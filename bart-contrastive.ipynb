{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()  # Initiate nltk lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_tokenize(sentence):\n",
    "    \"\"\" Simple function for tokenizing text with nltk \"\"\"\n",
    "    return nltk.word_tokenize(sentence, preserve_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from filelock import FileLock\n",
    "from transformers import AdamW, get_scheduler, set_seed\n",
    "\n",
    "from transformers.file_utils import is_offline_mode\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "# from args import parse_args\n",
    "# from data_loader import raw_data_loader, data_processor\n",
    "from model_loader import model_loader\n",
    "from rouge_s import py_rouge_scores\n",
    "from utils import label_smoothed_nll_loss, postprocess_text\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MODEL_MAPPING,\n",
    "    SchedulerType,\n",
    ")\n",
    "\n",
    "# You should update this to your particular problem to have better documentation of `model_type`\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--topic_tagger'], dest='topic_tagger', nargs=None, const=None, default=None, type=<class 'bool'>, choices=None, required=False, help='Use topic tag [TAG] or not', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "arg_parser = argparse.ArgumentParser(description=\"BART\")\n",
    "arg_parser.add_argument(\"--len_input\", dest=\"len_input\", type=str, default=None, help=\"set up prefix input\",choices=('no', 'topic', 'length', 'topic-length', 'length-topic', 'simple', 'simple-topic-tagger', 'simple-tagger'))\n",
    "arg_parser.add_argument(\"--len_output\", dest=\"len_output\", default=None, help=\"Use the ctrlen model or not\", choices=('no', 'topic', 'length', 'topic-length', 'length-topic'))\n",
    "arg_parser.add_argument(\"--output_dir\", dest=\"output_dir\", type=str, default=\"./output/1\", help=\"default\")\n",
    "arg_parser.add_argument(\"--train_file\", dest=\"train_file\", type=str, default=None, help=\"A csv or a json file containing the training data.\")\n",
    "arg_parser.add_argument(\"--validation_file\", dest=\"validation_file\", type=str, default=None, help=\"A csv or a json file containing the validation data.\")\n",
    "arg_parser.add_argument(\"--test_file\", dest=\"test_file\", type=str, default=None, help=\"A csv or a json file containing the test data.\")\n",
    "arg_parser.add_argument(\"--ignore_pad_token_for_loss\", dest=\"ignore_pad_token_for_loss\", type=bool, default=True, help=\"Whether to ignore the tokens corresponding to \" \"padded labels in the loss computation or not.\",)\n",
    "arg_parser.add_argument(\"--text_column\", dest=\"text_column\", type=str, default=\"dialogue\", help=\"The name of the column in the datasets containing the full texts (for summarization).\")\n",
    "arg_parser.add_argument(\"--summary_column\", dest=\"summary_column\", type=str, default=\"summary\", help=\"The name of the column in the datasets containing the summaries (for summarization).\")\n",
    "arg_parser.add_argument(\"--model_name_or_path\", dest=\"model_name_or_path\", type=str, default=\"facebook/bart-large\", help=\"Path to pretrained model or model identifier from huggingface.co/models.\")\n",
    "arg_parser.add_argument(\"--model_type\", dest=\"model_type\", type=str, default=\"bart\", help=\"Model type to use if training from scratch.\", choices=MODEL_TYPES)\n",
    "arg_parser.add_argument(\"--max_source_length\", dest=\"max_source_length\", type=int, default=1024, help=\"default\")\n",
    "arg_parser.add_argument(\"--source_prefix\", dest=\"source_prefix\", type=str, default=None, help=\"A prefix to add before every source text \" \"(useful for T5 models).\")\n",
    "arg_parser.add_argument(\"--preprocessing_num_workers\", type=int, default=None, help=\"The number of processes to use for the preprocessing.\")\n",
    "# arg_parser.add_argument(\"--overwrite_cache\", dest=\"overwrite_cache\", type=lambda x:bool(strtobool(x)), default=True, help=\"default\")\n",
    "arg_parser.add_argument(\"--overwrite_cache\", dest=\"overwrite_cache\", type=bool, default=None, help=\"Overwrite the cached training and evaluation sets\")\n",
    "arg_parser.add_argument(\"--min_target_length\", dest=\"min_target_length\", type=int, default=1, help=\"The minimal total sequence length for target text\")\n",
    "arg_parser.add_argument(\"--max_target_length\", dest=\"max_target_length\", type=int, default=128, help=\"The maximum total sequence length for target text after \"\n",
    "        \"tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "        \"during ``evaluate`` and ``predict``.\")\n",
    "arg_parser.add_argument(\"--num_beams\", dest=\"num_beams\", type=int, default=4, help=\"Number of beams to use for evaluation. This argument will be \"\n",
    "        \"passed to ``model.generate``, which is used during ``evaluate`` and ``predict``.\")\n",
    "arg_parser.add_argument(\"--learning_rate\", dest=\"learning_rate\", type=float, default=5e-5, help=\"Initial learning rate (after the potential warmup period) to use.\")\n",
    "arg_parser.add_argument(\"--pad_to_max_length\", action=\"store_true\", help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.\",)\n",
    "arg_parser.add_argument(\"--weight_decay\", dest=\"weight_decay\", type=float, default=1e-3, help=\"Weight decay to use.\")\n",
    "arg_parser.add_argument(\"--label_smoothing\", dest=\"label_smoothing\", type=float, default=0.1, help=\"hyperparameter for label smoothing.\")\n",
    "arg_parser.add_argument(\"--length_penalty\", dest=\"length_penalty\", type=float, default=1.0, help=\"large - longer sequence, small - shorter sequence\")\n",
    "arg_parser.add_argument(\"--num_train_epochs\", dest=\"num_train_epochs\", type=int, default=15, help=\"Total number of training epochs to perform.\")\n",
    "arg_parser.add_argument(\"--per_device_train_batch_size\", dest=\"per_device_train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\")\n",
    "arg_parser.add_argument(\"--gradient_accumulation_steps\", dest=\"gradient_accumulation_steps\", type=int, default=64, help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "arg_parser.add_argument(\"--per_device_eval_batch_size\", dest=\"per_device_eval_batch_size\", type=int, default=8, help=\"Batch size (per device) for the evaluation dataloader.\")\n",
    "arg_parser.add_argument(\"--per_device_test_batch_size\", dest=\"per_device_test_batch_size\", type=int, default=8, help=\"Batch size (per device) for the evaluation dataloader.\")\n",
    "arg_parser.add_argument(\"--num_warmup_steps\", dest=\"num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\")\n",
    "arg_parser.add_argument(\"--cache_dir\", dest=\"cache_dir\", type=str, default=\"./output/cache\", help=\"default\")\n",
    "arg_parser.add_argument(\"--seed\", dest=\"seed\", type=int, default=12345, help=\"default\")\n",
    "# arg_parser.add_argument(\"-f\", required=False) #important\n",
    "arg_parser.add_argument(\"--config_name\", type=str, default=None, help=\"Pretrained config name or path if not the same as model_name\")\n",
    "arg_parser.add_argument(\"--tokenizer_name\", type=str, default=None, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "arg_parser.add_argument(\"--use_slow_tokenizer\", dest=\"use_slow_tokenizer\", action=\"store_true\", help=\"If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).\")\n",
    "arg_parser.add_argument(\"--max_train_steps\", type=int, default=None, help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\")\n",
    "arg_parser.add_argument(\"--lr_scheduler_type\", type=SchedulerType, default=\"linear\", help=\"The scheduler type to use.\", choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"])\n",
    "arg_parser.add_argument(\"--ctrlen_model\", action='store_true', default=False, help=\"Use the ctrlen model or not\")\n",
    "arg_parser.add_argument(\"--sim_window_size\", type=int, default=5, help=\"window size for computing loss.\")\n",
    "arg_parser.add_argument(\"--sim_loss\", type=float, default=0, help=\"the loss weight for similarity scores.\")\n",
    "arg_parser.add_argument(\"--special_len_token_init\", type=str, default=None, help=\"ways to initialize special token for length (random, zero, token_embs)\")\n",
    "arg_parser.add_argument(\"--embedding_lr\", type=float, default=5e-5, help=\"Initial learning rate for embedding layers.\")\n",
    "arg_parser.add_argument(\"--len_start\", type=int, default=1, help=\"start length.\")\n",
    "arg_parser.add_argument(\"--len_end\", type=int, default=100, help=\"end length.\")\n",
    "arg_parser.add_argument(\"--data_aug\",action='store_true',default=False,help=\"whether to perform data augmentation or not\")\n",
    "arg_parser.add_argument(\"--pred_len\", action='store_true', default=False, help=\"whether to use the golden length or predicted length\")\n",
    "arg_parser.add_argument(\"--shuffle\", action='store_true', default=False, help=\"whether to shuffle the dataset to balance train/validation/test\")\n",
    "arg_parser.add_argument(\"--debug\", action='store_true', default=False, help=\"Use the debug mode or not\")\n",
    "\n",
    "arg_parser.add_argument(\"--topic_tagger\", dest=\"topic_tagger\", type=bool, default=None, help=\"Use topic tag [TAG] or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = arg_parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.train_file = \"./data/dialogtest/dialogsum.train.jsonl\"\n",
    "args.validation_file = \"./data/dialogtest/dialogsum.dev.jsonl\"\n",
    "args.test_file = \"./data/dialogtest/dialogsum.test.jsonl\"\n",
    "args.text_column = \"dialogue\"\n",
    "args.summary_column = \"summary\"\n",
    "args.model_name_or_path = \"facebook/bart-large\"\n",
    "args.model_type = \"bart\"\n",
    "args.max_source_length = 1024\n",
    "args.min_target_length = 1\n",
    "args.max_target_length = 128\n",
    "args.num_beams = 4\n",
    "args.learning_rate = 5e-5\n",
    "args.weight_decay = 1e-3\n",
    "args.label_smoothing = 0.1\n",
    "args.length_penalty = 1.0 \n",
    "args.num_train_epochs = 3\n",
    "args.per_device_train_batch_size = 2 \n",
    "args.gradient_accumulation_steps = 64 \n",
    "args.per_device_eval_batch_size = 8 \n",
    "args.per_device_test_batch_size = 8 \n",
    "args.num_warmup_steps = 0 \n",
    "args.cache_dir = \"./output/cache\"\n",
    "args.overwrite_cache = True\n",
    "args.seed = 12345\n",
    "\n",
    "args.len_input = 'topic-length'\n",
    "args.len_output = 'no'\n",
    "args.output_dir = \"./output/1-bart-baseline-loss\"\n",
    "\n",
    "args.topic_tagger = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic-length\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(args.len_input)\n",
    "print(args.topic_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_tokenize(sentence):\n",
    "    \"\"\" Simple function for tokenizing text with nltk \"\"\"\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def nltk_to_pos(pos):\n",
    "    \"\"\" Simple function for converting nltk pos to wordnet pos\"\"\"\n",
    "    if pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\" Function to lemmatize text according to the wordnet POS of each token \"\"\"\n",
    "\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    POS_assigned_text = nltk.pos_tag(tokenized_text)\n",
    "\n",
    "    available_POS = map(lambda x: (x[0], nltk_to_pos(x[1])), POS_assigned_text)\n",
    "\n",
    "    lemmatized_text = [token if pos is None\n",
    "                       else lemmatizer.lemmatize(token, pos)\n",
    "                       for token, pos in available_POS]\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "def build_tagger(original_tokens,lemmatized_tokens, topic_list, idx):\n",
    "    tagged_tokens = []\n",
    "    # Extract all the seed words according to the corresponding topic\n",
    "    token_topics = topic_list\n",
    "    original_list = original_tokens[idx]\n",
    "\n",
    "    for j, token in enumerate(lemmatized_tokens[idx]):\n",
    "        # If the lemmatized form of the token is in topic seeds, tag the original token\n",
    "        if token.lower() in token_topics:\n",
    "            # print(token.lower())\n",
    "            if token.lower() not in stopwords.words('english'):\n",
    "                # print(\"=\"*100)\n",
    "                # print(token.lower())\n",
    "                original_list[j] = '[TAG]' + original_list[j] + '[TAG]'\n",
    "\n",
    "    tagged_tokens.append(\" \".join(original_list))\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_from_dialogsum(args, file_path):\n",
    "    ''' load dialoguesum jsonl data '''\n",
    "\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    id_list = [sample['fname'] for sample in data]\n",
    "    dialogue_list = [sample['dialogue'] for sample in data]\n",
    "\n",
    "    if 'summary' in data[0]:\n",
    "        # summary\n",
    "        summary_list = [sample['summary'] for sample in data]\n",
    "        # topic\n",
    "        topic_list = [sample['topic'] for sample in data]\n",
    "\n",
    "    elif 'summary1' in data[0]:\n",
    "\n",
    "        id_list1 = [id+\"_sum1\" for id in id_list]\n",
    "        id_list2 = [id+\"_sum2\" for id in id_list]\n",
    "        id_list3 = [id+\"_sum3\" for id in id_list]\n",
    "\n",
    "        id_list = id_list1 + id_list2 + id_list3\n",
    "        dialogue_list = dialogue_list + dialogue_list + dialogue_list\n",
    "\n",
    "        # summary\n",
    "        summary_list1 = [sample['summary1'] for sample in data]\n",
    "        summary_list2 = [sample['summary2'] for sample in data]\n",
    "        summary_list3 = [sample['summary3'] for sample in data]\n",
    "\n",
    "        summary_list = summary_list1 + summary_list2 + summary_list3\n",
    "\n",
    "        # topic\n",
    "        topic_list1 = [sample['topic1'] for sample in data]\n",
    "        topic_list2 = [sample['topic2'] for sample in data]\n",
    "        topic_list3 = [sample['topic3'] for sample in data]\n",
    "\n",
    "        topic_list = topic_list1 + topic_list2 + topic_list3\n",
    "        \n",
    "    negative_topic_list = []\n",
    "    for topic in topic_list:\n",
    "        negative_topic = random.choice(topic_list)\n",
    "        if negative_topic == topic:\n",
    "            negative_topic = random.choice(negative_topic)\n",
    "        negative_topic_list.append(negative_topic)\n",
    "        \n",
    "\n",
    "    if args.topic_tagger:\n",
    "        topic_tagger = []\n",
    "        original_tokens = [simple_tokenize(x) for x in dialogue_list]\n",
    "        lemmatized_tokens = [lemmatize_text(x) for x in dialogue_list]\n",
    "        for i in range(len(lemmatized_tokens)):\n",
    "            tagger = build_tagger(original_tokens, lemmatized_tokens, topic_list[i], i)\n",
    "            topic_tagger.extend(tagger)\n",
    "\n",
    "        data_dict = {'id': id_list,\n",
    "                     'dialogue': topic_tagger,\n",
    "                     'summary': summary_list,\n",
    "                     'topic': topic_list}\n",
    "    else:\n",
    "        data_dict = {'id': id_list,\n",
    "                     'dialogue': dialogue_list,\n",
    "                     'summary': summary_list,\n",
    "                     'topic': topic_list,\n",
    "                     'negative_topic': negative_topic_list}\n",
    "\n",
    "    data_dict = Dataset.from_dict(data_dict)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dict = load_from_dialogsum(args, args.train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'topic', 'negative_topic'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do a favor'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['topic'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'redecoration'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['negative_topic'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(train_dict['topic'][i] != train_dict['negative_topic'][i] for i in range(len(train_dict['topic'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def raw_data_loader(args):\n",
    "    ''' load raw datasets from csv files '''\n",
    "\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    if args.test_file is not None:\n",
    "        data_files[\"test\"] = args.test_file\n",
    "\n",
    "    if 'dialogsum' in args.train_file:\n",
    "        train_dict = load_from_dialogsum(args, args.train_file)\n",
    "        val_dict   = load_from_dialogsum(args, args.validation_file)\n",
    "        test_dict  = load_from_dialogsum(args, args.test_file)\n",
    "\n",
    "    train_dict = utils.len_adjust(args, train_dict, 'train')\n",
    "    val_dict   = utils.len_adjust(args, val_dict, 'val')\n",
    "    test_dict  = utils.len_adjust(args, test_dict, 'test')\n",
    "\n",
    "    raw_datasets = datasets.DatasetDict({\"train\":train_dict, \"validation\":val_dict, \"test\":test_dict})\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogsum = raw_data_loader(args)\n",
    "dialogsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/17/2023 13:39:56 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "logger.info(accelerator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "accelerator.is_local_main_process\n",
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "set_seed(args.seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "accelerator.is_main_process\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/vocab.json\n",
      "loading file merges.txt from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/merges.txt\n",
      "loading file tokenizer.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50265. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Configuration saved in ./output/1-bart-baseline-loss/start/config.json\n",
      "Configuration saved in ./output/1-bart-baseline-loss/start/generation_config.json\n",
      "Model weights saved in ./output/1-bart-baseline-loss/start/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/1-bart-baseline-loss/start/tokenizer_config.json\n",
      "Special tokens file saved in ./output/1-bart-baseline-loss/start/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "config, tokenizer, model = model_loader(accelerator, logger, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_datasets = raw_data_loader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Topic of Summary: vaccines. Length of Summary: 18. Dialogue: #Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['dialogue'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Topic of Summary: get dressed. Length of Summary: 18. Dialogue: #Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['negative_dialogue'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n",
    "    def __init__(self, tokenizer, model, label_pad_token_id, pad_to_multiple_of):\n",
    "        super().__init__(tokenizer, model, label_pad_token_id, pad_to_multiple_of)\n",
    "        # self.padding_strategy = PaddingStrategy(padding_strategy)\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        batch = super().__call__(examples)\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        negative_input_ids = batch['negative_input_ids']\n",
    "        # negative_attention_mask = batch['negative_attention_mask']\n",
    "        decoder_input_ids = batch['decoder_input_ids']\n",
    "        labels = ['labels']\n",
    "\n",
    "        # Create lists to hold negative inputs and attention masks\n",
    "        # negative_input_ids = []\n",
    "        # negative_attention_mask = []\n",
    "        \n",
    "        max_len_input_ids = max([len(a) for a in input_ids])\n",
    "        max_len_negative_input_ids = max([len(a) for a in input_ids])\n",
    "\n",
    "#         self.tokenizer.encode(\n",
    "#                 random_input,\n",
    "#                 padding=self.padding_strategy,\n",
    "#                 max_length=self.model.config.max_position_embeddings,\n",
    "#             )\n",
    "#             random_attention_mask = [1] * len(random_input_ids)\n",
    "\n",
    "#             negative_input_ids.append(random_input_ids)\n",
    "#             negative_attention_mask.append(random_attention_mask)\n",
    "\n",
    "\n",
    "        return batch\n",
    "        \n",
    "    def collate_batch(self, features):\n",
    "        # Extract and add the \"id\" field to each batch\n",
    "        negative_input_ids = [feature[self.negative_input_ids] for feature in features]\n",
    "        # negative_attention_mask = [feature[self.negative_attention_mask] for feature in features]\n",
    "        batch = super().collate_batch(features)\n",
    "        batch[\"negative_input_ids\"] = negative_input_ids\n",
    "        # batch[\"negative_attention_mask\"] = negative_attention_mask\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_processor(logger, args, accelerator, raw_datasets, tokenizer, model):\n",
    "    ''' prepare dataset format for train/val/test '''\n",
    "    def preprocess_function(examples):\n",
    "        positive_dialogue = examples['dialogue']\n",
    "        negative_dialogue = examples['negative_dialogue']\n",
    "        topic_list = examples['topic']\n",
    "        summary_list = examples['summary']\n",
    "        # negative_topic_list = examples['negative_dialogue']\n",
    "\n",
    "#         input_length = tokenizer(positive_dialogue_list).input_ids\n",
    "#         negative_length = tokenizer(negative_dialogue_list).input_ids\n",
    "\n",
    "#         # Find the maximum length of sublists\n",
    "#         input_max_length = max(len(sublist) for sublist in input_length)\n",
    "#         negative_max_length = max(len(sublist) for sublist in negative_length)\n",
    "\n",
    "#         if input_max_length > negative_max_length and input_max_length < 1024:\n",
    "#             max_length = input_max_length\n",
    "#         elif input_max_length > negative_max_length and input_max_length > 1024:\n",
    "#             max_length = 1024\n",
    "#         elif input_max_length < negative_max_length and input_max_length < 1024:\n",
    "#             max_length = negative_max_length\n",
    "#         else:\n",
    "#             max_length = 1024\n",
    "\n",
    "        # Use the tokenizer to convert the input texts to model inputs\n",
    "        input_batch = tokenizer(\n",
    "        positive_dialogue,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=args.max_source_length,\n",
    "        return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # # Use the tokenizer to convert the input texts to model inputs\n",
    "        negative_batch = tokenizer(\n",
    "        negative_dialogue,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=args.max_source_length,\n",
    "        return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        # Prepare the labels (decoder input)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                summary_list,  # Assuming all examples have the same target text\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=args.max_target_length,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "\n",
    "        # input_batch['negative_input_ids'] = negative_batch['input_ids']\n",
    "        # input_batch['negative_attention_mask'] = negative_batch['attention_mask']\n",
    "        # input_batch['negative_dialogue'] = negative_dialogue\n",
    "        input_batch['input_ids'] = negative_batch['input_ids']\n",
    "        input_batch['attention_mask'] = negative_batch['attention_mask']\n",
    "        input_batch['negative_input_ids'] = negative_batch['input_ids']\n",
    "        input_batch['negative_attention_mask'] = negative_batch['attention_mask']\n",
    "        input_batch['decoder_input_ids'] = labels['input_ids']\n",
    "        input_batch['labels'] = labels['input_ids']\n",
    "        \n",
    "        return input_batch\n",
    "\n",
    "    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "    # Get the column names for input/target.\n",
    "    text_column = args.text_column\n",
    "    if text_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--text_column' value '{args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "    summary_column = args.summary_column\n",
    "    if summary_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--summary_column' value '{args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "    # Temporarily set max_target_length for training.\n",
    "    max_target_length = args.max_target_length\n",
    "    padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        processed_datasets = raw_datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset  = processed_datasets[\"validation\"]\n",
    "    test_dataset  = processed_datasets[\"test\"]\n",
    "\n",
    "    # label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    # ['longest', 'max_length', 'do_not_pad']\n",
    "    label_pad_token_id = tokenizer.pad_token_id\n",
    "    data_collator = CustomDataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id='do_not_pad',\n",
    "        pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.per_device_test_batch_size)\n",
    "\n",
    "    return (train_dataloader, eval_dataloader, test_dataloader), (train_dataset, eval_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abb5ecaa16d451e8543630fd41c81ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92d08de043546d9a5ba39ba78fb4ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20dd1c3102649aebd29e473612762cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:523: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader, processed_dataset = data_processor(logger, args, accelerator, raw_datasets, tokenizer, model)\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataloader\n",
    "train_dataset, _, _ = processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'negative_input_ids', 'negative_attention_mask', 'decoder_input_ids', 'labels'],\n",
       "     num_rows: 1500\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'negative_input_ids', 'negative_attention_mask', 'decoder_input_ids', 'labels'],\n",
       "     num_rows: 50\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'negative_input_ids', 'negative_attention_mask', 'decoder_input_ids', 'labels'],\n",
       "     num_rows: 150\n",
       " }))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = train_dataset['input_ids'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.pad({\"input_ids\": train_dataset['input_ids'][0]}, padding=\"max_length\", max_length=1024, return_tensors='pt', pad_to_multiple_of=8)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:736\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 736\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:708\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 943 at dim 1 (got 1024)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# print(batch)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids: \u001b[39m\u001b[38;5;124m'\u001b[39m, batch\u001b[38;5;241m.\u001b[39minput_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask: \u001b[39m\u001b[38;5;124m'\u001b[39m, batch\u001b[38;5;241m.\u001b[39mattention_mask[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 7\u001b[0m, in \u001b[0;36mCustomDataCollatorForSeq2Seq.__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples):\n\u001b[0;32m----> 7\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:586\u001b[0m, in \u001b[0;36mDataCollatorForSeq2Seq.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m             feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([remainder, feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m--> 586\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# prepare decoder_input_ids\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    596\u001b[0m     labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_decoder_input_ids_from_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    599\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3099\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3096\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3097\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3099\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    207\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:752\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    751\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 752\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    753\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    754\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    755\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    756\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    757\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    # print(batch)\n",
    "    print('input_ids: ', batch.input_ids[0].shape)\n",
    "    print('attention_mask: ', batch.attention_mask[0].shape)\n",
    "    print('negative_input_ids: ', batch.negative_input_ids[0].shape)\n",
    "    print('negative_attention_mask: ', batch.negative_attention_mask[0].shape)\n",
    "    print('labels: ', batch.labels[0].shape)\n",
    "    print('input_ids: ', batch.input_ids[1].shape)\n",
    "    print('attention_mask: ', batch.attention_mask[1].shape)\n",
    "    print('negative_input_ids: ', batch.negative_input_ids[1].shape)\n",
    "    print('negative_attention_mask: ', batch.negative_attention_mask[1].shape)\n",
    "    print('labels: ', batch.labels[1].shape)\n",
    "    print('='*100)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# = = = Training Preparation = = =\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "if args.ctrlen_model: \n",
    "    no_decay_emb_matrix = [\"bias\", \"LayerNorm.weight\", \"shared\"]\n",
    "else:\n",
    "    no_decay_emb_matrix = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay_emb_matrix)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "if args.ctrlen_model:\n",
    "    if args.model_type == 'bart': \n",
    "        optimizer_grouped_parameters.extend([{\n",
    "            \"params\": model.seq2seq_model.model.shared.parameters(),\n",
    "            \"lr\": args.embedding_lr}])\n",
    "    elif args.model_type == 't5':\n",
    "        optimizer_grouped_parameters.extend([{\n",
    "            \"params\": model.seq2seq_model.shared.parameters(),\n",
    "            \"lr\": args.embedding_lr}])\n",
    "    else:\n",
    "        raise ValueError('{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "# optimizer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2023 11:37:12 - INFO - __main__ - ***** Running training *****\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Num examples = 1500\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Num Epochs = 3\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Instantaneous batch size per device = 2\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Gradient Accumulation steps = 64\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Total optimization steps = 36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bcd4cbb8f64b169f29e2e8f613f4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")\n",
    "\n",
    "# = = = = = = = = = = = = = = = = Train = = = = = = = = = = = = = = = = = = =\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\" Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\" Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\" Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\" Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\" Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\" Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), desc=\"Training: \", disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "\n",
    "val_results = []\n",
    "acc_losses  = []\n",
    "best_r2_f1  = None\n",
    "best_epoch  = 0\n",
    "\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError('{} model type not implemented'.format(args.model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2023 11:37:12 - INFO - __main__ - ***** Running training *****\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Num examples = 1500\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Num Epochs = 3\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Instantaneous batch size per device = 2\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Gradient Accumulation steps = 64\n",
      "10/08/2023 11:37:12 - INFO - __main__ -  Total optimization steps = 36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e0f6a3114342e488a88d6048c7a6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "10/08/2023 11:37:55 - INFO - __main__ - \n",
      "10/08/2023 11:37:55 - INFO - __main__ - Rouge score on val set after epoch 1\n",
      "10/08/2023 11:37:55 - INFO - root - \n",
      "10/08/2023 11:37:55 - INFO - root - \trouge-1:\tP: 54.73\tR: 45.16\tF1: 48.24\n",
      "10/08/2023 11:37:55 - INFO - root - \trouge-2:\tP: 28.10\tR: 22.36\tF1: 24.16\n",
      "10/08/2023 11:37:55 - INFO - root - \trouge-l:\tP: 55.91\tR: 47.55\tF1: 50.47\n",
      "10/08/2023 11:37:55 - INFO - root - \n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/config.json\n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/generation_config.json\n",
      "Model weights saved in ./output/1-bart-baseline-loss/best/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/1-bart-baseline-loss/best/tokenizer_config.json\n",
      "Special tokens file saved in ./output/1-bart-baseline-loss/best/special_tokens_map.json\n",
      "10/08/2023 11:38:10 - INFO - __main__ - Current Best Validation Result is at epoch 1\n",
      "10/08/2023 11:38:10 - INFO - root - \n",
      "10/08/2023 11:38:10 - INFO - root - \trouge-1:\tP: 54.73\tR: 45.16\tF1: 48.24\n",
      "10/08/2023 11:38:10 - INFO - root - \trouge-2:\tP: 28.10\tR: 22.36\tF1: 24.16\n",
      "10/08/2023 11:38:10 - INFO - root - \trouge-l:\tP: 55.91\tR: 47.55\tF1: 50.47\n",
      "10/08/2023 11:38:10 - INFO - root - \n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "10/08/2023 11:38:53 - INFO - __main__ - \n",
      "10/08/2023 11:38:53 - INFO - __main__ - Rouge score on val set after epoch 2\n",
      "10/08/2023 11:38:53 - INFO - root - \n",
      "10/08/2023 11:38:53 - INFO - root - \trouge-1:\tP: 53.90\tR: 48.86\tF1: 49.61\n",
      "10/08/2023 11:38:53 - INFO - root - \trouge-2:\tP: 27.25\tR: 24.08\tF1: 24.65\n",
      "10/08/2023 11:38:53 - INFO - root - \trouge-l:\tP: 55.29\tR: 50.96\tF1: 51.85\n",
      "10/08/2023 11:38:53 - INFO - root - \n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/config.json\n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/generation_config.json\n",
      "Model weights saved in ./output/1-bart-baseline-loss/best/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/1-bart-baseline-loss/best/tokenizer_config.json\n",
      "Special tokens file saved in ./output/1-bart-baseline-loss/best/special_tokens_map.json\n",
      "10/08/2023 11:39:08 - INFO - __main__ - Current Best Validation Result is at epoch 2\n",
      "10/08/2023 11:39:08 - INFO - root - \n",
      "10/08/2023 11:39:08 - INFO - root - \trouge-1:\tP: 53.90\tR: 48.86\tF1: 49.61\n",
      "10/08/2023 11:39:08 - INFO - root - \trouge-2:\tP: 27.25\tR: 24.08\tF1: 24.65\n",
      "10/08/2023 11:39:08 - INFO - root - \trouge-l:\tP: 55.29\tR: 50.96\tF1: 51.85\n",
      "10/08/2023 11:39:08 - INFO - root - \n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "10/08/2023 11:39:42 - INFO - __main__ - \n",
      "10/08/2023 11:39:42 - INFO - __main__ - Rouge score on val set after epoch 3\n",
      "10/08/2023 11:39:42 - INFO - root - \n",
      "10/08/2023 11:39:42 - INFO - root - \trouge-1:\tP: 52.96\tR: 46.47\tF1: 48.11\n",
      "10/08/2023 11:39:42 - INFO - root - \trouge-2:\tP: 25.98\tR: 22.52\tF1: 23.41\n",
      "10/08/2023 11:39:42 - INFO - root - \trouge-l:\tP: 54.72\tR: 48.83\tF1: 50.59\n",
      "10/08/2023 11:39:42 - INFO - root - \n",
      "10/08/2023 11:39:42 - INFO - __main__ - Current Best Validation Result is at epoch 2\n",
      "10/08/2023 11:39:42 - INFO - root - \n",
      "10/08/2023 11:39:42 - INFO - root - \trouge-1:\tP: 53.90\tR: 48.86\tF1: 49.61\n",
      "10/08/2023 11:39:42 - INFO - root - \trouge-2:\tP: 27.25\tR: 24.08\tF1: 24.65\n",
      "10/08/2023 11:39:42 - INFO - root - \trouge-l:\tP: 55.29\tR: 50.96\tF1: 51.85\n",
      "10/08/2023 11:39:42 - INFO - root - \n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = = = = Train = = = = = = = = = = = = = = = = = = =\n",
    "total_batch_size = args.per_device_train_batch_size * \\\n",
    "    accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\" Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\" Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(\n",
    "    f\" Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(\n",
    "    f\" Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(\n",
    "    f\" Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\" Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), desc=\"Training: \",\n",
    "                    disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "\n",
    "val_results = []\n",
    "acc_losses = []\n",
    "best_r2_f1 = None\n",
    "best_epoch = 0\n",
    "\n",
    "# edit #\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    # task_specific_params = model.module.config.task_specific_params\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    # model.module.config.update(params)\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        '{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "loss_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "last_output = None\n",
    "hidden_states = None\n",
    "\n",
    "# =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = Train =  =  =  =  =  =  =  =  =  =  =  =  =  =  =\n",
    "for epoch in range(args.num_train_epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if args.ctrlen_model:  # CTRLen model\n",
    "            outputs, loss = model(batch, tokenizer)\n",
    "        # w/ and w/o label smoothing (always better with label smoothing)\n",
    "        else:\n",
    "            if args.label_smoothing == 0:\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            else:\n",
    "                outputs = model(**batch, output_hidden_states=True)\n",
    "                last_output = outputs\n",
    "                output_logits = outputs.logits\n",
    "                hidden_states = outputs.decoder_hidden_states\n",
    "                # print(f\"logits: {output_logits.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "                # # print(f\"hidden states: {hidden_states.shape}\")\n",
    "                # print(f\"loss: {outputs.loss}\")\n",
    "                # print(outputs.keys())\n",
    "                # print(\"=\"*100)\n",
    "                output_probs = torch.nn.functional.log_softmax(\n",
    "                    output_logits, dim=-1)\n",
    "                # edit #\n",
    "                # output_probs = output_probs.view(-1,\n",
    "                #                                  model.module.config.vocab_size)\n",
    "                output_probs = output_probs.view(-1,\n",
    "                                                 model.config.vocab_size)\n",
    "\n",
    "                gt_logits = batch['labels']\n",
    "                # print(f\"label: {gt_logits.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "                gt_logits = gt_logits.view(-1)\n",
    "\n",
    "                # print(f\"output_probs: {output_probs.shape}\")\n",
    "                # print(\"-\"*100)\n",
    "                # print(f\"gt_logits: {gt_logits.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "                \n",
    "                loss_nll, nll = label_smoothed_nll_loss(\n",
    "                    output_probs, gt_logits, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "                \n",
    "                cosine_loss = torch.nn.CosineEmbeddingLoss()\n",
    "                \n",
    "                loss_cs = cosine_loss(outputs.encoder_last_hidden_state[0], outputs.encoder_last_hidden_state[1], torch.ones(outputs.encoder_last_hidden_state.size(dim=1)).to(torch.device('cuda')))\n",
    "                \n",
    "                alpha = 0.5\n",
    "                \n",
    "                loss = loss_nll + alpha * (1 - loss_cs)\n",
    "                \n",
    "                # print(f\"loss_fn: {loss}\")\n",
    "                # print(\"-\"*100)\n",
    "                # print(f\"nll: {nll}\")\n",
    "                # print(\"=\"*100)\n",
    "\n",
    "        acc_losses.append(loss.item())\n",
    "        loss_list.append(loss)\n",
    "        epoch_loss += loss.item()\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        # print(f\"loss_grad: {loss}\")\n",
    "        accelerator.backward(loss)\n",
    "        # break\n",
    "\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix(lr=lr_scheduler.get_last_lr()[\n",
    "                                     0], loss=np.mean(acc_losses[-50:]))\n",
    "            completed_steps += 1\n",
    "            train_loss_list.append(epoch_loss/len(batch))\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    # # =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = EVAL =  =  =  =  =  =  =  =  =  =  =  =  =  =  =\n",
    "    model.eval()\n",
    "    val_predict = []\n",
    "    val_groundtruth = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            # print(generated_tokens)\n",
    "            # print(\"=\"*100)\n",
    "            \n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            \n",
    "            # print(generated_tokens)\n",
    "            # print(\"=\"*100)\n",
    "            \n",
    "            labels = batch[\"labels\"]\n",
    "            if not args.pad_to_max_length:\n",
    "                # If we did not pad to max length, we need to pad the labels too\n",
    "                labels = accelerator.pad_across_processes(\n",
    "                    batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "            \n",
    "            # print(generated_tokens)\n",
    "            # print(\"=\"*100)\n",
    "            # print(labels)\n",
    "            # print(\"=\"*100)\n",
    "            # loss, _ = label_smoothed_nll_loss_gen(generated_tokens, labels, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "            # break    \n",
    "            # loss, _ = map(label_smoothed_nll_loss, generated_tokens, labels, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "            # val_loss.extend(loss)\n",
    "            \n",
    "            generated_tokens = accelerator.gather(\n",
    "                generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            if args.ignore_pad_token_for_loss:\n",
    "                # Replace -100 in the labels as we can't decode them.\n",
    "                labels = np.where(labels != -100, labels,\n",
    "                                tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "\n",
    "            # print(generated_tokens[0])\n",
    "            # print(\"=\"*100)\n",
    "            # print(labels[0])\n",
    "            # loss, _ = label_smoothed_nll_loss(generated_tokens[0], labels[0], args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "            # break \n",
    "            \n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(\n",
    "                labels, skip_special_tokens=True)\n",
    "            \n",
    "            # print(decoded_preds)\n",
    "            # print(\"=\"*100)\n",
    "            # print(decoded_labels)\n",
    "            # print(\"=\"*100)\n",
    "            \n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels)\n",
    "\n",
    "            # print(decoded_preds)\n",
    "            # print(\"=\"*100)\n",
    "            # print(decoded_labels)\n",
    "            # print(\"=\"*100)\n",
    "            # if step == 1:\n",
    "            #     break\n",
    "\n",
    "            val_predict.extend(decoded_preds)\n",
    "            val_groundtruth.extend(decoded_labels)\n",
    "\n",
    "    if args.len_output == 'real':\n",
    "        new_val_predict = []\n",
    "        for sample in val_predict:\n",
    "            try:\n",
    "                gen_sum = sample.split('Summary: ')[2]\n",
    "                new_val_predict.append(gen_sum)\n",
    "            except:\n",
    "                new_val_predict.append(sample)\n",
    "        val_predict = new_val_predict\n",
    "    else:\n",
    "        new_val_predict = val_predict\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Rouge score on val set after epoch {}\".format(epoch+1))\n",
    "    eval_results = py_rouge_scores(val_predict, val_groundtruth)\n",
    "\n",
    "    if best_r2_f1 is None:\n",
    "        best_r2_f1 = eval_results\n",
    "    if eval_results['rouge-2']['f'] >= best_r2_f1['rouge-2']['f']:\n",
    "        best_r2_f1 = eval_results\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "        os.makedirs(args.output_dir+'/best', exist_ok=True)\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            args.output_dir+'/best', save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir+'/best')\n",
    "        # save vocab\n",
    "        vocab = tokenizer.vocab.copy()\n",
    "        vocab = {k: v for k, v in sorted(\n",
    "            vocab.items(), key=lambda item: item[1])}\n",
    "        with open(args.output_dir + '/best/vocab.txt', 'w') as f:\n",
    "            for word, index in vocab.items():\n",
    "                # it lead to encoding bug on some machines, so i add this line\n",
    "                word = word.encode('ascii', 'ignore').decode('ascii')\n",
    "                f.write(str(index) + ': ' + word + '\\n')\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    logger.info(\n",
    "        \"Current Best Validation Result is at epoch {}\".format(best_epoch))\n",
    "    py_rouge_scores(None, None, best_r2_f1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.encoder_last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4340e-02,  3.4438e-02,  3.4452e-02,  ..., -8.5761e-03,\n",
       "           2.4686e-03, -2.0838e-03],\n",
       "         [-1.3822e-01, -4.3015e-02,  2.6078e-01,  ..., -1.4171e-01,\n",
       "          -1.0360e-01, -2.5367e-01],\n",
       "         [-5.5475e-04,  1.2314e-02,  7.7116e-03,  ...,  5.5711e-03,\n",
       "          -3.1495e-03,  3.5236e-04],\n",
       "         ...,\n",
       "         [-1.9751e-03,  1.7929e-02,  9.2430e-03,  ...,  4.5511e-03,\n",
       "           8.8687e-04, -5.6736e-04],\n",
       "         [-1.0416e-02,  6.9710e-03,  2.3643e-02,  ...,  5.7164e-03,\n",
       "           5.1921e-03, -4.1290e-03],\n",
       "         [-8.8324e-04,  1.6706e-02,  9.7642e-03,  ...,  5.1859e-03,\n",
       "          -4.2920e-03,  4.3312e-04]],\n",
       "\n",
       "        [[ 2.3276e-02,  3.6314e-02,  3.2427e-02,  ..., -7.2097e-03,\n",
       "          -1.4225e-02, -8.0660e-03],\n",
       "         [-1.6026e-01, -2.3157e-02,  2.5404e-01,  ..., -4.2496e-02,\n",
       "          -2.6232e-02, -2.0289e-01],\n",
       "         [-3.6479e-02, -5.3065e-01, -4.0594e-02,  ...,  2.8221e-02,\n",
       "          -5.6595e-02, -3.1839e-01],\n",
       "         ...,\n",
       "         [-1.8181e-02,  6.9863e-03,  6.6897e-03,  ..., -4.1762e-04,\n",
       "          -2.4568e-03, -1.2915e-02],\n",
       "         [-6.5732e-04,  1.5399e-02,  1.0035e-02,  ...,  6.2574e-03,\n",
       "          -5.5280e-03, -3.1648e-05],\n",
       "         [-1.3892e-02,  2.1192e-02,  2.0377e-02,  ...,  3.3729e-03,\n",
       "           1.1347e-02, -8.5512e-03]]], device='cuda:0',\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.encoder_last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.encoder_last_hidden_state.size(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 1024])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.decoder_hidden_states[-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.decoder_hidden_states[-1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([32, 50266])\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs.logits))\n",
    "print(outputs.logits[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_loss = torch.nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.decoder_hidden_states[-1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_cs = cosine_loss(outputs.decoder_hidden_states[-1][0], outputs.decoder_hidden_states[-1][1], torch.ones(outputs.decoder_hidden_states[-1].shape[1]).to(torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4350, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -1 * torch.ones(outputs.decoder_hidden_states[-1].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cs = cosine_loss(outputs.decoder_hidden_states[-1][0], outputs.decoder_hidden_states[-1][1], x.to(torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5650, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4350, device='cuda:0', grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - loss_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/08/2023 11:39:42 - INFO - __main__ - Loading Best Result is at epoch 2 for Testing\n",
      "loading configuration file ./output/1-bart-baseline-loss/best/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 1,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 1,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50266\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading weights file ./output/1-bart-baseline-loss/best/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./output/1-bart-baseline-loss/best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./output/1-bart-baseline-loss/best/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "10/08/2023 11:39:45 - INFO - __main__ - Collecting Testing Result...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "10/08/2023 11:39:57 - INFO - __main__ - \n",
      "10/08/2023 11:39:57 - INFO - __main__ - ROUGE score on test set\n",
      "10/08/2023 11:39:57 - INFO - root - \n",
      "10/08/2023 11:39:57 - INFO - root - \trouge-1:\tP: 39.50\tR: 44.67\tF1: 40.52\n",
      "10/08/2023 11:39:57 - INFO - root - \trouge-2:\tP: 12.53\tR: 14.24\tF1: 12.88\n",
      "10/08/2023 11:39:57 - INFO - root - \trouge-l:\tP: 36.75\tR: 40.73\tF1: 37.72\n",
      "10/08/2023 11:39:57 - INFO - root - \n",
      "10/08/2023 11:39:57 - INFO - __main__ - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic of Summary: communication method. Length of Summary: 27. Dialogue: # Person1 # : Ms. Dawson , I need you to take a dictation for me . # Person2 # : Yes , sir ... # Person1 # : This should go out as an intra-office memorandum to all employees by this afternoon . Are you ready ? # Person2 # : Yes , sir . Go ahead . # Person1 # : Attention all staff ... Effective immediately , all office [TAG]communications[TAG] are restricted to email correspondence and official memos . The use of Instant Message programs by employees during working hours is strictly prohibited . # Person2 # : Sir , does this apply to intra-office [TAG]communications[TAG] only ? Or will it also restrict external [TAG]communications[TAG] ? # Person1 # : It should apply to all [TAG]communications[TAG] , not only in this office between employees , but also any outside [TAG]communications[TAG] . # Person2 # : But sir , many employees use Instant Messaging to communicate with their clients . # Person1 # : They will just have to change their [TAG]communication[TAG] [TAG]methods[TAG] . I do n't want any - one using Instant Messaging in this office . It wastes too much time ! Now , please continue with the memo . Where were we ? # Person2 # : This applies to internal and external [TAG]communications[TAG] . # Person1 # : Yes . Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation . At second offense , the employee will face termination . Any questions regarding this new policy may be directed to department heads . # Person2 # : Is that all ? # Person1 # : Yes . Please get this memo typed up and distributed to all employees before 4 pm .\n"
     ]
    }
   ],
   "source": [
    "# =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = Test =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = \n",
    "# load best model\n",
    "logger.info(\"Loading Best Result is at epoch {} for Testing\".format(best_epoch))\n",
    "\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "config          = config.from_pretrained(args.output_dir+'/best')\n",
    "tokenizer       = tokenizer.from_pretrained(args.output_dir+'/best', config=config)\n",
    "unwrapped_model = unwrapped_model.from_pretrained(args.output_dir+'/best', config=config)\n",
    "model           = accelerator.prepare(unwrapped_model)\n",
    "\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError('{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "# start Test \n",
    "logger.info(\"Collecting Testing Result...\")\n",
    "model.eval()\n",
    "\n",
    "test_predict     = []\n",
    "test_groundtruth = []\n",
    "for step, batch in enumerate(tqdm(test_dataloader, leave=False)):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        if not args.pad_to_max_length:\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "        generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "        labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "        if args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "\n",
    "        decoded_preds  = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        decoded_preds  = [' '.join(sent.split('\\n')) for sent in decoded_preds]\n",
    "        decoded_labels = [' '.join(sent.split('\\n')) for sent in decoded_labels]\n",
    "\n",
    "        test_predict.extend(decoded_preds)\n",
    "        test_groundtruth.extend(decoded_labels)\n",
    "\n",
    "print(raw_datasets['test']['dialogue'][0])\n",
    "\n",
    "if args.len_output == 'real':\n",
    "    new_test_predict = []\n",
    "    for sample in test_predict:\n",
    "        try:\n",
    "            gen_sum = sample.split('Summary: ')[2]\n",
    "            new_test_predict.append(gen_sum)\n",
    "        except:\n",
    "            new_test_predict.append(sample)\n",
    "    test_predict = new_test_predict\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"ROUGE score on test set\")\n",
    "test_scores = py_rouge_scores(test_predict, test_groundtruth)\n",
    "logger.info(\"\")\n",
    "\n",
    "\n",
    "# Save generated summaries\n",
    "if args.len_input == 'predict':\n",
    "    os.makedirs(args.output_dir+'/predict_gen_samples', exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(args.output_dir+'/gen_samples', exist_ok=True)\n",
    "\n",
    "for i in range(len(test_predict)):\n",
    "    test_id        = raw_datasets['test']['id'][i]\n",
    "    test_dialogue  = raw_datasets['test']['dialogue'][i]\n",
    "    test_summary   = raw_datasets['test']['summary'][i]\n",
    "    test_predict_s = test_predict[i]\n",
    "\n",
    "    if args.len_input == 'predict':\n",
    "        with open(args.output_dir+'/predict_gen_samples/'+str(test_id)+'.txt', 'w') as f:\n",
    "            test_dialogue = test_dialogue.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_dialogue)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Golden Summary:\\n')\n",
    "            test_summary = test_summary.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_summary)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Generate Summary:\\n')\n",
    "            test_predict_s = test_predict_s.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_predict_s)\n",
    "    else:\n",
    "        with open(args.output_dir+'/gen_samples/'+str(test_id)+'.txt', 'w') as f:\n",
    "            test_dialogue = test_dialogue.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_dialogue)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Golden Summary:\\n')\n",
    "            test_summary = test_summary.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_summary)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Generate Summary:\\n')\n",
    "            test_predict_s = test_predict_s.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_predict_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
