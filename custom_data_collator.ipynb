{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfGuU-bBsu3J",
    "outputId": "e19cfe61-e01a-4e25-f7d9-d5f86adeedfd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lDTyTH6ysu3O",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()  # Initiate nltk lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "id": "BDHK-dEEsu3P",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from filelock import FileLock\n",
    "from transformers import AdamW, get_scheduler, set_seed\n",
    "\n",
    "from transformers.file_utils import is_offline_mode\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from args import parse_args\n",
    "from data_loader import raw_data_loader, data_processor\n",
    "from model_loader import model_loader\n",
    "from rouge_s import py_rouge_scores\n",
    "from utils import label_smoothed_nll_loss, postprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dZmO8rfYsu3Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zI3YKK_psu3Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MODEL_MAPPING,\n",
    "    SchedulerType,\n",
    ")\n",
    "\n",
    "# You should update this to your particular problem to have better documentation of `model_type`\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AD495S_Dsu3S",
    "outputId": "ac6a6306-5a6b-4f3b-ca58-d8d40e6c20cf",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--alpha'], dest='alpha', nargs=None, const=None, default=0.5, type=<class 'float'>, choices=None, required=False, help='ration of computation loss in encoder_hidden_layer', metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "arg_parser = argparse.ArgumentParser(description=\"BART\")\n",
    "arg_parser.add_argument(\"--len_input\", dest=\"len_input\", type=str, default=None, help=\"set up prefix input\",choices=('no', 'topic', 'length', 'topic-length', 'length-topic', 'simple', 'simple-topic-tagger', 'simple-tagger'))\n",
    "arg_parser.add_argument(\"--len_output\", dest=\"len_output\", default=None, help=\"Use the ctrlen model or not\", choices=('no', 'topic', 'length', 'topic-length', 'length-topic'))\n",
    "arg_parser.add_argument(\"--output_dir\", dest=\"output_dir\", type=str, default=\"./output/1\", help=\"default\")\n",
    "arg_parser.add_argument(\"--train_file\", dest=\"train_file\", type=str, default=None, help=\"A csv or a json file containing the training data.\")\n",
    "arg_parser.add_argument(\"--validation_file\", dest=\"validation_file\", type=str, default=None, help=\"A csv or a json file containing the validation data.\")\n",
    "arg_parser.add_argument(\"--test_file\", dest=\"test_file\", type=str, default=None, help=\"A csv or a json file containing the test data.\")\n",
    "arg_parser.add_argument(\"--ignore_pad_token_for_loss\", dest=\"ignore_pad_token_for_loss\", type=bool, default=True, help=\"Whether to ignore the tokens corresponding to \" \"padded labels in the loss computation or not.\",)\n",
    "arg_parser.add_argument(\"--text_column\", dest=\"text_column\", type=str, default=\"dialogue\", help=\"The name of the column in the datasets containing the full texts (for summarization).\")\n",
    "arg_parser.add_argument(\"--summary_column\", dest=\"summary_column\", type=str, default=\"summary\", help=\"The name of the column in the datasets containing the summaries (for summarization).\")\n",
    "arg_parser.add_argument(\"--model_name_or_path\", dest=\"model_name_or_path\", type=str, default=\"facebook/bart-large\", help=\"Path to pretrained model or model identifier from huggingface.co/models.\")\n",
    "arg_parser.add_argument(\"--model_type\", dest=\"model_type\", type=str, default=\"bart\", help=\"Model type to use if training from scratch.\", choices=MODEL_TYPES)\n",
    "arg_parser.add_argument(\"--max_source_length\", dest=\"max_source_length\", type=int, default=1024, help=\"default\")\n",
    "arg_parser.add_argument(\"--source_prefix\", dest=\"source_prefix\", type=str, default=None, help=\"A prefix to add before every source text \" \"(useful for T5 models).\")\n",
    "arg_parser.add_argument(\"--preprocessing_num_workers\", type=int, default=None, help=\"The number of processes to use for the preprocessing.\")\n",
    "# arg_parser.add_argument(\"--overwrite_cache\", dest=\"overwrite_cache\", type=lambda x:bool(strtobool(x)), default=True, help=\"default\")\n",
    "arg_parser.add_argument(\"--overwrite_cache\", dest=\"overwrite_cache\", type=bool, default=None, help=\"Overwrite the cached training and evaluation sets\")\n",
    "arg_parser.add_argument(\"--min_target_length\", dest=\"min_target_length\", type=int, default=1, help=\"The minimal total sequence length for target text\")\n",
    "arg_parser.add_argument(\"--max_target_length\", dest=\"max_target_length\", type=int, default=128, help=\"The maximum total sequence length for target text after \"\n",
    "        \"tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "        \"during ``evaluate`` and ``predict``.\")\n",
    "arg_parser.add_argument(\"--num_beams\", dest=\"num_beams\", type=int, default=4, help=\"Number of beams to use for evaluation. This argument will be \"\n",
    "        \"passed to ``model.generate``, which is used during ``evaluate`` and ``predict``.\")\n",
    "arg_parser.add_argument(\"--learning_rate\", dest=\"learning_rate\", type=float, default=5e-5, help=\"Initial learning rate (after the potential warmup period) to use.\")\n",
    "arg_parser.add_argument(\"--pad_to_max_length\", action=\"store_true\", help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.\",)\n",
    "arg_parser.add_argument(\"--weight_decay\", dest=\"weight_decay\", type=float, default=1e-3, help=\"Weight decay to use.\")\n",
    "arg_parser.add_argument(\"--label_smoothing\", dest=\"label_smoothing\", type=float, default=0.1, help=\"hyperparameter for label smoothing.\")\n",
    "arg_parser.add_argument(\"--length_penalty\", dest=\"length_penalty\", type=float, default=1.0, help=\"large - longer sequence, small - shorter sequence\")\n",
    "arg_parser.add_argument(\"--num_train_epochs\", dest=\"num_train_epochs\", type=int, default=15, help=\"Total number of training epochs to perform.\")\n",
    "arg_parser.add_argument(\"--per_device_train_batch_size\", dest=\"per_device_train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\")\n",
    "arg_parser.add_argument(\"--gradient_accumulation_steps\", dest=\"gradient_accumulation_steps\", type=int, default=64, help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "arg_parser.add_argument(\"--per_device_eval_batch_size\", dest=\"per_device_eval_batch_size\", type=int, default=8, help=\"Batch size (per device) for the evaluation dataloader.\")\n",
    "arg_parser.add_argument(\"--per_device_test_batch_size\", dest=\"per_device_test_batch_size\", type=int, default=8, help=\"Batch size (per device) for the evaluation dataloader.\")\n",
    "arg_parser.add_argument(\"--num_warmup_steps\", dest=\"num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\")\n",
    "arg_parser.add_argument(\"--cache_dir\", dest=\"cache_dir\", type=str, default=\"./output/cache\", help=\"default\")\n",
    "arg_parser.add_argument(\"--seed\", dest=\"seed\", type=int, default=12345, help=\"default\")\n",
    "# arg_parser.add_argument(\"-f\", required=False) #important\n",
    "arg_parser.add_argument(\"--config_name\", type=str, default=None, help=\"Pretrained config name or path if not the same as model_name\")\n",
    "arg_parser.add_argument(\"--tokenizer_name\", type=str, default=None, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "arg_parser.add_argument(\"--use_slow_tokenizer\", dest=\"use_slow_tokenizer\", action=\"store_true\", help=\"If passed, will use a slow tokenizer (not backed by the ðŸ¤— Tokenizers library).\")\n",
    "arg_parser.add_argument(\"--max_train_steps\", type=int, default=None, help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\")\n",
    "arg_parser.add_argument(\"--lr_scheduler_type\", type=SchedulerType, default=\"linear\", help=\"The scheduler type to use.\", choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"])\n",
    "arg_parser.add_argument(\"--ctrlen_model\", action='store_true', default=False, help=\"Use the ctrlen model or not\")\n",
    "arg_parser.add_argument(\"--sim_window_size\", type=int, default=5, help=\"window size for computing loss.\")\n",
    "arg_parser.add_argument(\"--sim_loss\", type=float, default=0, help=\"the loss weight for similarity scores.\")\n",
    "arg_parser.add_argument(\"--special_len_token_init\", type=str, default=None, help=\"ways to initialize special token for length (random, zero, token_embs)\")\n",
    "arg_parser.add_argument(\"--embedding_lr\", type=float, default=5e-5, help=\"Initial learning rate for embedding layers.\")\n",
    "arg_parser.add_argument(\"--len_start\", type=int, default=1, help=\"start length.\")\n",
    "arg_parser.add_argument(\"--len_end\", type=int, default=100, help=\"end length.\")\n",
    "arg_parser.add_argument(\"--data_aug\",action='store_true',default=False,help=\"whether to perform data augmentation or not\")\n",
    "arg_parser.add_argument(\"--pred_len\", action='store_true', default=False, help=\"whether to use the golden length or predicted length\")\n",
    "arg_parser.add_argument(\"--shuffle\", action='store_true', default=False, help=\"whether to shuffle the dataset to balance train/validation/test\")\n",
    "arg_parser.add_argument(\"--debug\", action='store_true', default=False, help=\"Use the debug mode or not\")\n",
    "\n",
    "arg_parser.add_argument(\"--topic_tagger\", dest=\"topic_tagger\", type=bool, default=None, help=\"Use topic tag <TAG> or not\")\n",
    "arg_parser.add_argument(\"--contrastive_loss\", dest=\"contrastive_loss\", type=bool, default=False, help=\"Use contrastive loss or not\")\n",
    "arg_parser.add_argument(\"--alpha\", dest=\"alpha\", type=float, default=0.5, help=\"ration of computation loss in encoder_hidden_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sizQFLtFsu3T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = arg_parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-nJe4mEqsu3U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.train_file = \"./data/dialogtest/dialogsum.train.jsonl\"\n",
    "args.validation_file = \"./data/dialogtest/dialogsum.dev.jsonl\"\n",
    "args.test_file = \"./data/dialogtest/dialogsum.test.jsonl\"\n",
    "args.text_column = \"dialogue\"\n",
    "args.summary_column = \"summary\"\n",
    "args.model_name_or_path = \"facebook/bart-large\"\n",
    "args.model_type = \"bart\"\n",
    "args.max_source_length = 1024\n",
    "args.min_target_length = 1\n",
    "args.max_target_length = 128\n",
    "args.num_beams = 4\n",
    "args.learning_rate = 5e-5\n",
    "args.weight_decay = 1e-3\n",
    "args.label_smoothing = 0.1\n",
    "args.length_penalty = 1.0\n",
    "args.num_train_epochs = 5\n",
    "args.per_device_train_batch_size = 2\n",
    "args.gradient_accumulation_steps = 64\n",
    "args.per_device_eval_batch_size = 8\n",
    "args.per_device_test_batch_size = 8\n",
    "args.num_warmup_steps = 0\n",
    "args.cache_dir = \"./output/cache\"\n",
    "args.overwrite_cache = True\n",
    "args.seed = 12345\n",
    "\n",
    "args.len_input = 'topic-length'\n",
    "args.len_output = 'no'\n",
    "args.output_dir = \"./output/1-bart-baseline-loss\"\n",
    "\n",
    "args.topic_tagger = True\n",
    "args.contrastive_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOgOOeeYsu3U",
    "outputId": "bea7709d-f0e9-44b8-b4e7-f9a50bde9a46",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic-length\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(args.len_input)\n",
    "print(args.topic_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tz2ZuJAMsu3X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_datasets = raw_data_loader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXOyCtjYvQUH",
    "outputId": "f15f305c-71b5-4c0f-adc7-074cad71fee8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Topic of Summary: <TAG>discuss stress</TAG>. Length of Summary: 30. Dialogue: # Person1 # : Hi , Mr. Smith . I 'm Doctor Hawkins . Why are you here today ? # Person2 # : I found it would be a good idea to <TAG>get</TAG> a <TAG>check-up</TAG> . # Person1 # : Yes , well , you have n't had one for 5 years . You should have one every year . # Person2 # : I know . I figure as long as there is nothing wrong , why go see the doctor ? # Person1 # : Well , the best way to avoid serious illnesses is to find out about them early . So try to come at least once a year for your own good . # Person2 # : Ok. # Person1 # : Let me see here . Your eyes and ears look fine . Take a deep breath , please . Do you smoke , Mr. Smith ? # Person2 # : Yes . # Person1 # : Smoking is the leading cause of lung cancer and heart disease , you know . You really should quit . # Person2 # : I 've tried hundreds of times , but I just ca n't seem to kick the habit . # Person1 # : Well , we have classes and some medications that might help . I 'll give you more information before you leave . # Person2 # : Ok , thanks doctor .\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['negative_dialogue'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LVH7c2csu3X",
    "outputId": "df3ae011-014b-437e-a6ab-8d285a7c979c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/22/2023 13:06:12 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "logger.info(accelerator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JrjSjmUssu3Y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "accelerator.is_local_main_process\n",
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "set_seed(args.seed)\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "accelerator.is_main_process\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9y_Gspbqsu3Y",
    "outputId": "aac050f9-7369-4b98-f0fa-343cfa962896",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/vocab.json\n",
      "loading file merges.txt from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/merges.txt\n",
      "loading file tokenizer.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at ./output/cache/models--facebook--bart-large/snapshots/cb48c1365bd826bd521f650dc2e0940aee54720c/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50267. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Configuration saved in ./output/1-bart-baseline-loss/start/config.json\n",
      "Configuration saved in ./output/1-bart-baseline-loss/start/generation_config.json\n",
      "Model weights saved in ./output/1-bart-baseline-loss/start/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/1-bart-baseline-loss/start/tokenizer_config.json\n",
      "Special tokens file saved in ./output/1-bart-baseline-loss/start/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "config, tokenizer, model = model_loader(accelerator, logger, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_auto_class', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_check_and_enable_flash_attn_2', '_convert_head_mask_to_5d', '_copy_lm_head_original_to_resized', '_create_repo', '_dispatch_accelerate_model', '_expand_inputs_for_generation', '_extract_past_from_model_output', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_from_config', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_decoder_start_token_id', '_get_files_timestamps', '_get_generation_mode', '_get_logits_processor', '_get_logits_warper', '_get_name', '_get_resized_embeddings', '_get_resized_lm_head', '_get_stopping_criteria', '_hf_peft_config_loaded', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_weights', '_initialize_weights', '_is_full_backward_hook', '_is_hf_initialized', '_keep_in_fp32_modules', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_pretrained_model', '_load_pretrained_model_low_mem', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_initialize_input_ids_for_generation', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_modules', '_named_members', '_no_split_modules', '_non_persistent_buffers_set', '_parameters', '_prepare_attention_mask_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_model_inputs', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_final_logits_bias', '_resize_token_embeddings', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_skip_keys_device_placement', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_supports_flash_attn_2', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_tied_weights_keys', '_update_model_kwargs_for_generation', '_upload_modified_files', '_validate_generated_length', '_validate_model_class', '_validate_model_kwargs', '_version', 'active_adapter', 'active_adapters', 'add_adapter', 'add_memory_hooks', 'add_module', 'apply', 'assisted_decoding', 'base_model', 'base_model_prefix', 'beam_sample', 'beam_search', 'bfloat16', 'buffers', 'call_super_init', 'can_generate', 'children', 'compute_transition_scores', 'config', 'config_class', 'constrained_beam_search', 'contrastive_search', 'cpu', 'create_extended_attention_mask_for_decoder', 'cuda', 'device', 'disable_adapters', 'disable_input_require_grads', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'enable_adapters', 'enable_input_require_grads', 'estimate_tokens', 'eval', 'extra_repr', 'final_logits_bias', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'generation_config', 'get_adapter_state_dict', 'get_buffer', 'get_decoder', 'get_encoder', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'greedy_search', 'group_beam_search', 'half', 'init_weights', 'invert_attention_mask', 'ipu', 'is_gradient_checkpointing', 'is_loaded_in_4bit', 'is_loaded_in_8bit', 'is_parallelizable', 'lm_head', 'load_adapter', 'load_state_dict', 'main_input_name', 'model', 'modules', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters', 'post_init', 'prepare_decoder_input_ids_from_labels', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'reverse_bettertransformer', 'sample', 'save_pretrained', 'set_adapter', 'set_extra_state', 'set_input_embeddings', 'set_output_embeddings', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_bettertransformer', 'to_empty', 'train', 'training', 'type', 'vocab_size', 'warn_if_padding_and_no_attention_mask', 'warnings_issued', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50267"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPECIAL_TOKENS_ATTRIBUTES', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_tokens', '_additional_special_tokens', '_auto_class', '_batch_encode_plus', '_bos_token', '_call_one', '_cls_token', '_compile_jinja_template', '_convert_encoding', '_convert_id_to_token', '_convert_token_to_id_with_added_voc', '_create_repo', '_decode', '_decode_use_source_tokenizer', '_encode_plus', '_eos_token', '_eventual_warn_about_too_long_sequence', '_eventually_correct_t5_max_length', '_from_pretrained', '_get_files_timestamps', '_get_padding_truncation_strategies', '_in_target_context_manager', '_mask_token', '_pad', '_pad_token', '_pad_token_type_id', '_processor_class', '_save_pretrained', '_sep_token', '_set_processor_class', '_switch_to_input_mode', '_switch_to_target_mode', '_tokenizer', '_unk_token', '_upload_modified_files', 'add_prefix_space', 'add_special_tokens', 'add_tokens', 'added_tokens_decoder', 'added_tokens_encoder', 'additional_special_tokens', 'additional_special_tokens_ids', 'all_special_ids', 'all_special_tokens', 'all_special_tokens_extended', 'apply_chat_template', 'as_target_tokenizer', 'backend_tokenizer', 'batch_decode', 'batch_encode_plus', 'bos_token', 'bos_token_id', 'build_inputs_with_special_tokens', 'can_save_slow_tokenizer', 'chat_template', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id', 'convert_added_tokens', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'decoder', 'default_chat_template', 'deprecation_warnings', 'encode', 'encode_plus', 'eos_token', 'eos_token_id', 'from_pretrained', 'get_added_vocab', 'get_special_tokens_mask', 'get_vocab', 'init_inputs', 'init_kwargs', 'is_fast', 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'max_model_input_sizes', 'model_input_names', 'model_max_length', 'name_or_path', 'num_special_tokens_to_add', 'pad', 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_seq2seq_batch', 'pretrained_init_configuration', 'pretrained_vocab_files_map', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'sep_token', 'sep_token_id', 'set_truncation_and_padding', 'slow_tokenizer_class', 'special_tokens_map', 'special_tokens_map_extended', 'split_special_tokens', 'tokenize', 'train_new_from_iterator', 'truncate_sequences', 'truncation_side', 'unk_token', 'unk_token_id', 'verbose', 'vocab', 'vocab_files_names', 'vocab_size']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<TAG>', '</TAG>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 48931, 9, 19584, 35, 1437, 50265, 18909, 4781, 3992, 50266, 4, 41852, 9, 19584, 35, 389, 4, 33854, 35, 849, 18404, 134, 849, 4832, 12289, 2156, 427, 4, 1259, 479, 38, 128, 119, 12521, 15633, 479, 2612, 32, 47, 259, 452, 17487, 849, 18404, 176, 849, 4832, 38, 303, 24, 74, 28, 10, 205, 1114, 7, 1437, 50265, 6460, 50266, 10, 1437, 50265, 15954, 12, 658, 50266, 479, 849, 18404, 134, 849, 4832, 3216, 2156, 157, 2156, 47, 33, 295, 75, 56, 65, 13, 195, 107, 479, 370, 197, 33, 65, 358, 76, 479, 849, 18404, 176, 849, 4832, 38, 216, 479, 38, 1955, 25, 251, 25, 89, 16, 1085, 1593, 2156, 596, 213, 192, 5, 3299, 17487, 849, 18404, 134, 849, 4832, 2647, 2156, 5, 275, 169, 7, 1877, 1473, 14971, 16, 7, 465, 66, 59, 106, 419, 479, 407, 860, 7, 283, 23, 513, 683, 10, 76, 13, 110, 308, 205, 479, 849, 18404, 176, 849, 4832, 5148, 4, 849, 18404, 134, 849, 4832, 2780, 162, 192, 259, 479, 2486, 2473, 8, 12137, 356, 2051, 479, 4624, 10, 1844, 8016, 2156, 2540, 479, 1832, 47, 4603, 2156, 427, 4, 1259, 17487, 849, 18404, 176, 849, 4832, 3216, 479, 849, 18404, 134, 849, 4832, 38001, 16, 5, 981, 1303, 9, 10665, 1668, 8, 1144, 2199, 2156, 47, 216, 479, 370, 269, 197, 6602, 479, 849, 18404, 176, 849, 4832, 38, 128, 548, 1381, 2213, 9, 498, 2156, 53, 38, 95, 6056, 295, 75, 2045, 7, 3151, 5, 10870, 479, 849, 18404, 134, 849, 4832, 2647, 2156, 52, 33, 4050, 8, 103, 12102, 14, 429, 244, 479, 38, 128, 890, 492, 47, 55, 335, 137, 47, 989, 479, 849, 18404, 176, 849, 4832, 5148, 2156, 2446, 3299, 479, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(raw_datasets['train']['negative_dialogue'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n",
    "    def __init__(self, tokenizer, model, label_pad_token_id, pad_to_multiple_of, id_field=\"id\"):\n",
    "        super().__init__(tokenizer, model, label_pad_token_id, pad_to_multiple_of)\n",
    "        self.id_field = id_field\n",
    "\n",
    "    def collate_batch(self, features):\n",
    "        batch = super().collate_batch(features)\n",
    "\n",
    "        # Extract and add the \"id\" field to the batch\n",
    "        ids = [feature[self.id_field] for feature in features]\n",
    "        batch[\"id\"] = id\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "II9gKXJfsu3X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_processor(logger, args, accelerator, raw_datasets, tokenizer, model):\n",
    "    ''' prepare dataset format for train/val/test '''\n",
    "    def preprocess_function(examples):\n",
    "\n",
    "        # summary - target\n",
    "        targets = examples[summary_column]\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "        if args.ctrlen_model:\n",
    "            gold_sum_len = [len(item) for item in labels['attention_mask']]\n",
    "\n",
    "        # dialogue - input\n",
    "        inputs = examples[text_column]\n",
    "        new_inputs = []\n",
    "        for i, inp in enumerate(inputs):\n",
    "            if args.ctrlen_model:\n",
    "                if 'pred_len' in examples:\n",
    "                    new_inputs.append(prefix + \"<len_{}> \".format(examples['pred_len'][i]) + inp)\n",
    "\n",
    "                else:\n",
    "                    new_inputs.append(prefix + \"<len_{}> \".format(gold_sum_len[i]) + inp)\n",
    "            else:\n",
    "                new_inputs.append(prefix + inp)\n",
    "\n",
    "        inputs = new_inputs\n",
    "        id = examples['id']\n",
    "        new_id = [x.split(\"_\")[1] for x in id]\n",
    "        # print(new_id)\n",
    "        # print(type(new_id))\n",
    "        # negative_dialogue = examples['negative_dialogue']\n",
    "        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n",
    "        # negative_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n",
    "        # negative_inputs = tokenizer(negative_dialogue, max_length=args.max_source_length, padding=padding, truncation=True)\n",
    "        # print(model_inputs)\n",
    "        # print(len(model_inputs['input_ids'][0]))\n",
    "        # print(len(negative_inputs['input_ids'][0]))\n",
    "        # print(len(model_inputs['attention_mask']))\n",
    "\n",
    "        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "        # padding in the loss.\n",
    "        if padding == \"max_length\" and args.ignore_pad_token_for_loss:\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "\n",
    "        # model_inputs['neg_input_ids'] = negative_inputs['input_ids']\n",
    "        # model_inputs['neg_attention_mask'] = negative_inputs['attention_mask']\n",
    "        model_inputs[\"id\"] = new_id\n",
    "        # print(type(labels[\"input_ids\"]))\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "        if args.ctrlen_model:\n",
    "            model_inputs[\"gold_len\"] = gold_sum_len\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "    # Get the column names for input/target.\n",
    "    text_column = args.text_column\n",
    "    if text_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--text_column' value '{args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "    summary_column = args.summary_column\n",
    "    if summary_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--summary_column' value '{args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "    # Temporarily set max_target_length for training.\n",
    "    max_target_length = args.max_target_length\n",
    "    padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        processed_datasets = raw_datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset  = processed_datasets[\"validation\"]\n",
    "    test_dataset  = processed_datasets[\"test\"]\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 1):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    # data_collator = DataCollatorForSeq2Seq(\n",
    "    #     tokenizer,\n",
    "    #     model=model,\n",
    "    #     label_pad_token_id=label_pad_token_id,\n",
    "    #     pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    "    # )\n",
    "\n",
    "    data_collator = CustomDataCollatorForSeq2Seq(        \n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    "        # padding='do_not_pad'\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.per_device_test_batch_size)\n",
    "\n",
    "    return (train_dataloader, eval_dataloader, test_dataloader), (train_dataset, eval_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "d0098c03b82d48c9acdc684fbbf54567",
      "fa32d34f467944f2aaf4a13fdf7a0cf2",
      "e3bc145889c94fc3974e88e46863db89",
      "ddbc6421b4cf487d92cc53fb0ae6f634",
      "13bb8c4b9a154bb99c4d38cd2edb631c",
      "5a2947306125452c8c25c12e54dc6656",
      "326356876bd540778d188c41673e04f5",
      "542a00a050384f5f8f3f7354eb527336",
      "a91a8c5870234ab3bdd0132addb34a20",
      "9c182b108efa471994186923f10871a8",
      "c34934bec45646a8b2ac76c60dc124d5",
      "24272444846441638d4836fd835562dc",
      "78f6c1bca77640ca92bd87d5995fced5",
      "81669ba2443a489cb535c508333dc974",
      "47ddb2c2d8fa4c278c9d514ace68341f",
      "b842e08bc1ca45ee99a868d1dfa4d0db",
      "4616558f0ebd466babad0202bc55824d",
      "9a4ed651601f4a29b763eed10afafc16",
      "368a820467ad4f4d8198932125f0d357",
      "f5ece15e08e44905a09d9a2e988a4be4",
      "932e4629e7f340c2a5b64d095c46bf4d",
      "a26c7dc74d71494787fd32d560594b38",
      "90b25ecb208546f9938652c360ef7a5e",
      "852ba2b2a4fd4bbe80a2ecd9c9f6b5fb",
      "7f1272db213a4121ae95b5e41061d964",
      "a3ad3e93702041d89b6f505ec6d37411",
      "ffcb842d99954cdcaa8fb78e8e3b2215",
      "b1a15a93c5e746888806f8dd19cf6889",
      "35f50d433dc14aa28cf0f5a039f7cb87",
      "e69f5057741b46dba3ebb186ef85c6db",
      "14052c5abfad4231a0d4c8d0e0395a01",
      "3bad8094e9124ab391d945edf4ccca2f",
      "31409e259c8b4ee7899035ea4ba3f140"
     ]
    },
    "id": "BNku9V-ssu3Y",
    "outputId": "ab678f18-5bfa-4b54-fbb4-cac959f8edcb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0bc1a0dd2f45d4b22ade7abd4ae086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9598c49dc84489d99a8e64b982ef206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d1b17c8e194a7fba2cfd22af0579f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "10/14/2023 06:47:05 - INFO - __main__ - Sample 2649 of the training set: {'id': '2649', 'input_ids': [0, 48931, 9, 19584, 35, 81, 18288, 3588, 4, 41852, 9, 19584, 35, 883, 4, 33854, 35, 849, 41761, 134, 10431, 35, 407, 99, 109, 47, 206, 9, 127, 92, 3588, 116, 50118, 10431, 41761, 176, 10431, 35, 38, 206, 24, 18, 372, 328, 38, 269, 101, 5, 3195, 6, 8, 24, 1411, 269, 157, 19, 110, 2473, 4, 38, 348, 888, 57, 21754, 14, 3588, 23, 4471, 833, 18, 13, 10, 150, 122, 6, 53, 38, 938, 75, 686, 38, 115, 4960, 33035, 12, 9579, 1932, 13, 10, 3588, 23, 5, 1151, 4, 50118, 10431, 41761, 134, 10431, 35, 12698, 6, 33035, 12, 9579, 1932, 116, 50118, 10431, 41761, 176, 10431, 35, 3216, 6, 14, 18, 5, 425, 51, 58, 4579, 24, 23, 4, 38, 21, 2818, 24, 429, 213, 15, 1392, 1010, 4, 2612, 6, 141, 203, 222, 47, 582, 13, 24, 116, 50118, 10431, 41761, 134, 10431, 35, 38, 2162, 24, 13, 65, 6317, 8, 24503, 12, 9579, 1932, 23, 11668, 18, 23207, 5150, 4, 38, 399, 75, 216, 38, 115, 120, 24, 7246, 6152, 1493, 4, 50118, 10431, 41761, 176, 10431, 35, 1525, 768, 328, 9567, 23, 11668, 18, 16, 81, 18288, 6, 8, 51, 1331, 383, 14, 47, 64, 465, 23, 818, 358, 97, 5296, 1400, 328, 2612, 399, 75, 47, 2792, 198, 116, 50118, 10431, 41761, 134, 10431, 35, 38, 218, 75, 216, 6, 38, 95, 794, 5, 3588, 8, 802, 59, 141, 1256, 24, 1326, 8, 141, 203, 38, 770, 24, 4, 38, 4443, 38, 21, 350, 4023, 25170, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 10431, 41761, 134, 10431, 5684, 14, 849, 41761, 134, 10431, 2162, 41, 81, 18288, 3588, 23, 11668, 18, 23207, 5150, 71, 1576, 5, 754, 14, 5, 3588, 16, 33035, 12, 9579, 1932, 23, 4471, 833, 18, 4, 2]}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worachotn/.local/lib/python3.10/site-packages/accelerate/accelerator.py:523: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader, processed_dataset = data_processor(logger, args, accelerator, raw_datasets, tokenizer, model)\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataloader\n",
    "train_dataset, _, _ = processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HD4JVgBRFOlA",
    "outputId": "8915a69c-5801-4da1-9053-00859e286af5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12460\n",
       "})"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['id'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHvfVGcQvhIC",
    "outputId": "654ee714-48ee-4e49-cdb2-2a8cf6edbe0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f16fb5545e0>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "P7kZdADiv0dl",
    "outputId": "c7280b18-3968-4d9a-a408-1246043ea647"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "-100 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[287], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;66;03m# print(tokenizer.decode(batch['input_ids'][0]))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/data/data_collator.py:586\u001b[0m, in \u001b[0;36mDataCollatorForSeq2Seq.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m             feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([remainder, feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m--> 586\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# prepare decoder_input_ids\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    596\u001b[0m     labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_decoder_input_ids_from_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    599\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3263\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3260\u001b[0m         encoded_inputs[key] \u001b[38;5;241m=\u001b[39m to_py_obj(value)\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;66;03m# Convert padding_strategy in PaddingStrategy\u001b[39;00m\n\u001b[0;32m-> 3263\u001b[0m padding_strategy, _, max_length, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3267\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(required_input[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2648\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2646\u001b[0m     padding_strategy \u001b[38;5;241m=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mLONGEST  \u001b[38;5;66;03m# Default to pad to the longest sequence in the batch\u001b[39;00m\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, PaddingStrategy):\n\u001b[0;32m-> 2648\u001b[0m     padding_strategy \u001b[38;5;241m=\u001b[39m \u001b[43mPaddingStrategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, PaddingStrategy):\n\u001b[1;32m   2650\u001b[0m     padding_strategy \u001b[38;5;241m=\u001b[39m padding\n",
      "File \u001b[0;32m/usr/lib/python3.10/enum.py:385\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_(\n\u001b[1;32m    388\u001b[0m         value,\n\u001b[1;32m    389\u001b[0m         names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[1;32m    394\u001b[0m         )\n",
      "File \u001b[0;32m/usr/lib/python3.10/enum.py:718\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    717\u001b[0m             exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m ve_exc\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# ensure all variables that could hold an exception are destroyed\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/enum.py:700\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_missing_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    702\u001b[0m     exc \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:433\u001b[0m, in \u001b[0;36mExplicitEnum._missing_\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_missing_\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value):\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, please select one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_value2member_map_\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: -100 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "  # print(tokenizer.decode(batch['input_ids'][0]))\n",
    "  print(batch)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([33, 88, 51, 11, 82, 84, 32, 45, 52, 60])\n",
      "1 tensor([54, 36, 26, 34, 65, 21,  2, 29, 38, 99])\n",
      "2 tensor([59, 48, 25, 83, 17, 44, 96, 79, 49,  4])\n",
      "3 tensor([23, 19, 50, 42, 16, 80, 89, 24, 27, 63])\n",
      "4 tensor([93, 39,  0, 92, 64,  6, 94, 87, 18, 22])\n",
      "5 tensor([43, 85, 41, 66, 67, 86, 77, 20, 97, 10])\n",
      "6 tensor([71, 46, 72, 62, 91,  8, 15,  5, 98, 61])\n",
      "7 tensor([78, 70, 68,  9, 74, 81, 90, 56, 75, 73])\n",
      "8 tensor([30,  7,  1, 53, 37, 28,  3, 47, 57, 40])\n",
      "9 tensor([95, 12, 69, 55, 13, 76, 31, 14, 58, 35])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 tensor([ 4, 12, 27, 83, 42, 98, 87, 72, 51, 35])\n",
      "1 tensor([46,  9, 85, 28, 26, 56, 88, 94, 31, 74])\n",
      "2 tensor([49, 68, 44, 62, 54, 30, 79, 76, 90, 24])\n",
      "3 tensor([ 2,  8,  6, 22, 91, 92, 73, 66, 50, 17])\n",
      "4 tensor([95,  1, 57, 59,  5, 21, 53, 71, 16,  7])\n",
      "5 tensor([93, 19, 23, 61, 25,  3, 47, 99, 77, 69])\n",
      "6 tensor([63, 58, 38, 89, 81, 97, 67, 13, 41, 18])\n",
      "7 tensor([34, 75,  0, 84, 43, 52, 37, 60, 96, 33])\n",
      "8 tensor([40, 48, 86, 55, 39, 78, 32, 64, 82, 11])\n",
      "9 tensor([29, 80, 70, 15, 36, 45, 10, 14, 65, 20])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing the required libraries \n",
    "import torch \n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader \n",
    "  \n",
    "# defining the Dataset class \n",
    "class data_set(Dataset): \n",
    "    def __init__(self): \n",
    "        numbers = list(range(0, 100, 1)) \n",
    "        self.data = numbers \n",
    "  \n",
    "    def __len__(self): \n",
    "        return len(self.data) \n",
    "  \n",
    "    def __getitem__(self, index): \n",
    "        return self.data[index] \n",
    "  \n",
    "  \n",
    "dataset = data_set() \n",
    "dataset2 = data_set() \n",
    "  \n",
    "# implementing dataloader on the dataset and printing per batch \n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True) \n",
    "dataloader2 = DataLoader(dataset2, batch_size=10, shuffle=True) \n",
    "for i, batch in enumerate(dataloader): \n",
    "    print(i, batch) \n",
    "print(\"-\"*100)\n",
    "for i, batch in enumerate(dataloader2): \n",
    "    print(i, batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Unable to create tensor, you should probably activate truncation and/or padding with\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\" features (`{key}` in this case) have excessive nesting (inputs type `list` where type `int` is\" expected).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3z17N4Jsu3Y",
    "outputId": "1849c4bf-2915-4771-b728-98c0e23ab664",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# = = = Training Preparation = = =\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "if args.ctrlen_model:\n",
    "    no_decay_emb_matrix = [\"bias\", \"LayerNorm.weight\", \"shared\"]\n",
    "else:\n",
    "    no_decay_emb_matrix = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay_emb_matrix)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "if args.ctrlen_model:\n",
    "    if args.model_type == 'bart':\n",
    "        optimizer_grouped_parameters.extend([{\n",
    "            \"params\": model.seq2seq_model.model.shared.parameters(),\n",
    "            \"lr\": args.embedding_lr}])\n",
    "    elif args.model_type == 't5':\n",
    "        optimizer_grouped_parameters.extend([{\n",
    "            \"params\": model.seq2seq_model.shared.parameters(),\n",
    "            \"lr\": args.embedding_lr}])\n",
    "    else:\n",
    "        raise ValueError('{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "# optimizer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9dY-SZEsu3Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4b15de89357c428997a59f4e22166ad3"
     ]
    },
    "id": "JBQUGWIqsu3Z",
    "outputId": "062bdecd-ceb2-484a-ab79-13750ecfc0b7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/04/2023 18:57:09 - INFO - __main__ - ***** Running training *****\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Num examples = 1500\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Num Epochs = 5\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Instantaneous batch size per device = 2\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Gradient Accumulation steps = 64\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Total optimization steps = 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b15de89357c428997a59f4e22166ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")\n",
    "\n",
    "# = = = = = = = = = = = = = = = = Train = = = = = = = = = = = = = = = = = = =\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\" Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\" Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\" Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\" Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\" Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\" Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), desc=\"Training: \", disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "\n",
    "val_results = []\n",
    "acc_losses  = []\n",
    "best_r2_f1  = None\n",
    "best_epoch  = 0\n",
    "\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError('{} model type not implemented'.format(args.model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "c93c25e40e054458b7dfbd865069cadd"
     ]
    },
    "id": "awYeoAgOsu3Z",
    "outputId": "332daa87-c3c6-4fef-edcb-0e5124a48f23",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/04/2023 18:57:09 - INFO - __main__ - ***** Running training *****\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Num examples = 1500\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Num Epochs = 5\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Instantaneous batch size per device = 2\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Gradient Accumulation steps = 64\n",
      "10/04/2023 18:57:09 - INFO - __main__ -  Total optimization steps = 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93c25e40e054458b7dfbd865069cadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "10/04/2023 18:59:14 - INFO - __main__ - \n",
      "10/04/2023 18:59:14 - INFO - __main__ - Rouge score on val set after epoch 1\n",
      "10/04/2023 18:59:14 - INFO - root - \n",
      "10/04/2023 18:59:14 - INFO - root - \trouge-1:\tP: 52.18\tR: 46.22\tF1: 47.37\n",
      "10/04/2023 18:59:14 - INFO - root - \trouge-2:\tP: 26.84\tR: 23.33\tF1: 23.97\n",
      "10/04/2023 18:59:14 - INFO - root - \trouge-l:\tP: 53.10\tR: 47.65\tF1: 48.99\n",
      "10/04/2023 18:59:14 - INFO - root - \n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/config.json\n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/generation_config.json\n",
      "Model weights saved in ./output/1-bart-baseline-loss/best/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/1-bart-baseline-loss/best/tokenizer_config.json\n",
      "Special tokens file saved in ./output/1-bart-baseline-loss/best/special_tokens_map.json\n",
      "10/04/2023 18:59:21 - INFO - __main__ - Current Best Validation Result is at epoch 1\n",
      "10/04/2023 18:59:21 - INFO - root - \n",
      "10/04/2023 18:59:21 - INFO - root - \trouge-1:\tP: 52.18\tR: 46.22\tF1: 47.37\n",
      "10/04/2023 18:59:21 - INFO - root - \trouge-2:\tP: 26.84\tR: 23.33\tF1: 23.97\n",
      "10/04/2023 18:59:21 - INFO - root - \trouge-l:\tP: 53.10\tR: 47.65\tF1: 48.99\n",
      "10/04/2023 18:59:21 - INFO - root - \n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "10/04/2023 19:01:23 - INFO - __main__ - \n",
      "10/04/2023 19:01:23 - INFO - __main__ - Rouge score on val set after epoch 2\n",
      "10/04/2023 19:01:23 - INFO - root - \n",
      "10/04/2023 19:01:23 - INFO - root - \trouge-1:\tP: 51.86\tR: 46.98\tF1: 47.64\n",
      "10/04/2023 19:01:23 - INFO - root - \trouge-2:\tP: 25.87\tR: 23.43\tF1: 23.81\n",
      "10/04/2023 19:01:23 - INFO - root - \trouge-l:\tP: 53.01\tR: 48.84\tF1: 49.65\n",
      "10/04/2023 19:01:23 - INFO - root - \n",
      "10/04/2023 19:01:23 - INFO - __main__ - Current Best Validation Result is at epoch 1\n",
      "10/04/2023 19:01:23 - INFO - root - \n",
      "10/04/2023 19:01:23 - INFO - root - \trouge-1:\tP: 52.18\tR: 46.22\tF1: 47.37\n",
      "10/04/2023 19:01:23 - INFO - root - \trouge-2:\tP: 26.84\tR: 23.33\tF1: 23.97\n",
      "10/04/2023 19:01:23 - INFO - root - \trouge-l:\tP: 53.10\tR: 47.65\tF1: 48.99\n",
      "10/04/2023 19:01:23 - INFO - root - \n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "10/04/2023 19:03:29 - INFO - __main__ - \n",
      "10/04/2023 19:03:29 - INFO - __main__ - Rouge score on val set after epoch 3\n",
      "10/04/2023 19:03:29 - INFO - root - \n",
      "10/04/2023 19:03:29 - INFO - root - \trouge-1:\tP: 54.85\tR: 52.05\tF1: 51.87\n",
      "10/04/2023 19:03:29 - INFO - root - \trouge-2:\tP: 29.68\tR: 28.02\tF1: 27.99\n",
      "10/04/2023 19:03:29 - INFO - root - \trouge-l:\tP: 56.37\tR: 53.95\tF1: 53.99\n",
      "10/04/2023 19:03:29 - INFO - root - \n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/config.json\n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/generation_config.json\n",
      "Model weights saved in ./output/1-bart-baseline-loss/best/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/1-bart-baseline-loss/best/tokenizer_config.json\n",
      "Special tokens file saved in ./output/1-bart-baseline-loss/best/special_tokens_map.json\n",
      "10/04/2023 19:03:36 - INFO - __main__ - Current Best Validation Result is at epoch 3\n",
      "10/04/2023 19:03:36 - INFO - root - \n",
      "10/04/2023 19:03:36 - INFO - root - \trouge-1:\tP: 54.85\tR: 52.05\tF1: 51.87\n",
      "10/04/2023 19:03:36 - INFO - root - \trouge-2:\tP: 29.68\tR: 28.02\tF1: 27.99\n",
      "10/04/2023 19:03:36 - INFO - root - \trouge-l:\tP: 56.37\tR: 53.95\tF1: 53.99\n",
      "10/04/2023 19:03:36 - INFO - root - \n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "10/04/2023 19:05:39 - INFO - __main__ - \n",
      "10/04/2023 19:05:39 - INFO - __main__ - Rouge score on val set after epoch 4\n",
      "10/04/2023 19:05:39 - INFO - root - \n",
      "10/04/2023 19:05:39 - INFO - root - \trouge-1:\tP: 51.46\tR: 53.48\tF1: 50.99\n",
      "10/04/2023 19:05:39 - INFO - root - \trouge-2:\tP: 28.61\tR: 29.61\tF1: 28.15\n",
      "10/04/2023 19:05:39 - INFO - root - \trouge-l:\tP: 53.13\tR: 54.78\tF1: 52.88\n",
      "10/04/2023 19:05:39 - INFO - root - \n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/config.json\n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/generation_config.json\n",
      "Model weights saved in ./output/1-bart-baseline-loss/best/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/1-bart-baseline-loss/best/tokenizer_config.json\n",
      "Special tokens file saved in ./output/1-bart-baseline-loss/best/special_tokens_map.json\n",
      "10/04/2023 19:05:46 - INFO - __main__ - Current Best Validation Result is at epoch 4\n",
      "10/04/2023 19:05:46 - INFO - root - \n",
      "10/04/2023 19:05:46 - INFO - root - \trouge-1:\tP: 51.46\tR: 53.48\tF1: 50.99\n",
      "10/04/2023 19:05:46 - INFO - root - \trouge-2:\tP: 28.61\tR: 29.61\tF1: 28.15\n",
      "10/04/2023 19:05:46 - INFO - root - \trouge-l:\tP: 53.13\tR: 54.78\tF1: 52.88\n",
      "10/04/2023 19:05:46 - INFO - root - \n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "10/04/2023 19:07:04 - INFO - __main__ - \n",
      "10/04/2023 19:07:04 - INFO - __main__ - Rouge score on val set after epoch 5\n",
      "10/04/2023 19:07:04 - INFO - root - \n",
      "10/04/2023 19:07:04 - INFO - root - \trouge-1:\tP: 51.59\tR: 52.52\tF1: 50.69\n",
      "10/04/2023 19:07:04 - INFO - root - \trouge-2:\tP: 28.44\tR: 28.63\tF1: 27.67\n",
      "10/04/2023 19:07:04 - INFO - root - \trouge-l:\tP: 53.50\tR: 54.16\tF1: 52.83\n",
      "10/04/2023 19:07:04 - INFO - root - \n",
      "10/04/2023 19:07:04 - INFO - __main__ - Current Best Validation Result is at epoch 4\n",
      "10/04/2023 19:07:04 - INFO - root - \n",
      "10/04/2023 19:07:04 - INFO - root - \trouge-1:\tP: 51.46\tR: 53.48\tF1: 50.99\n",
      "10/04/2023 19:07:04 - INFO - root - \trouge-2:\tP: 28.61\tR: 29.61\tF1: 28.15\n",
      "10/04/2023 19:07:04 - INFO - root - \trouge-l:\tP: 53.13\tR: 54.78\tF1: 52.88\n",
      "10/04/2023 19:07:04 - INFO - root - \n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = = = = Train = = = = = = = = = = = = = = = = = = =\n",
    "total_batch_size = args.per_device_train_batch_size * \\\n",
    "    accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\" Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\" Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(\n",
    "    f\" Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(\n",
    "    f\" Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(\n",
    "    f\" Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\" Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), desc=\"Training: \",\n",
    "                    disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "\n",
    "val_results = []\n",
    "acc_losses = []\n",
    "best_r2_f1 = None\n",
    "best_epoch = 0\n",
    "\n",
    "# edit #\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    # task_specific_params = model.module.config.task_specific_params\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    # model.module.config.update(params)\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        '{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "loss_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "last_output = None\n",
    "hidden_states = None\n",
    "\n",
    "# =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = Train =  =  =  =  =  =  =  =  =  =  =  =  =  =  =\n",
    "for epoch in range(args.num_train_epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if args.ctrlen_model:  # CTRLen model\n",
    "            outputs, loss = model(batch, tokenizer)\n",
    "        # w/ and w/o label smoothing (always better with label smoothing)\n",
    "        else:\n",
    "            if args.label_smoothing == 0:\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            else:\n",
    "                outputs = model(**batch, output_hidden_states=True)\n",
    "                last_output = outputs\n",
    "                output_logits = outputs.logits\n",
    "                hidden_states = outputs.decoder_hidden_states\n",
    "                # print(f\"logits: {output_logits.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "                # # print(f\"hidden states: {hidden_states.shape}\")\n",
    "                # print(f\"loss: {outputs.loss}\")\n",
    "                # print(outputs.keys())\n",
    "                # print(\"=\"*100)\n",
    "                output_probs = torch.nn.functional.log_softmax(\n",
    "                    output_logits, dim=-1)\n",
    "                # edit #\n",
    "                # output_probs = output_probs.view(-1,\n",
    "                #                                  model.module.config.vocab_size)\n",
    "                output_probs = output_probs.view(-1,\n",
    "                                                 model.config.vocab_size)\n",
    "\n",
    "                gt_logits = batch['labels']\n",
    "                # print(f\"label: {gt_logits.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "                gt_logits = gt_logits.view(-1)\n",
    "\n",
    "                # print(f\"output_probs: {output_probs.shape}\")\n",
    "                # print(\"-\"*100)\n",
    "                # print(f\"gt_logits: {gt_logits.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "\n",
    "                loss_nll, nll = label_smoothed_nll_loss(\n",
    "                    output_probs, gt_logits, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "                cosine_loss = torch.nn.CosineEmbeddingLoss()\n",
    "\n",
    "                loss_cs = cosine_loss(outputs.encoder_last_hidden_state[0], outputs.encoder_last_hidden_state[1], torch.ones(outputs.encoder_last_hidden_state.size(dim=1)).to(torch.device('cuda')))\n",
    "\n",
    "                alpha = 0.5\n",
    "\n",
    "                loss = loss_nll + alpha * (1 - loss_cs)\n",
    "\n",
    "                # print(f\"loss_fn: {loss}\")\n",
    "                # print(\"-\"*100)\n",
    "                # print(f\"nll: {nll}\")\n",
    "                # print(\"=\"*100)\n",
    "\n",
    "        acc_losses.append(loss.item())\n",
    "        loss_list.append(loss)\n",
    "        epoch_loss += loss.item()\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        # print(f\"loss_grad: {loss}\")\n",
    "        accelerator.backward(loss)\n",
    "        # break\n",
    "\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix(lr=lr_scheduler.get_last_lr()[\n",
    "                                     0], loss=np.mean(acc_losses[-50:]))\n",
    "            completed_steps += 1\n",
    "            train_loss_list.append(epoch_loss/len(batch))\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    # # =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = EVAL =  =  =  =  =  =  =  =  =  =  =  =  =  =  =\n",
    "    model.eval()\n",
    "    val_predict = []\n",
    "    val_groundtruth = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            # print(generated_tokens)\n",
    "            # print(\"=\"*100)\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            # print(generated_tokens)\n",
    "            # print(\"=\"*100)\n",
    "\n",
    "            labels = batch[\"labels\"]\n",
    "            if not args.pad_to_max_length:\n",
    "                # If we did not pad to max length, we need to pad the labels too\n",
    "                labels = accelerator.pad_across_processes(\n",
    "                    batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "            # print(generated_tokens)\n",
    "            # print(\"=\"*100)\n",
    "            # print(labels)\n",
    "            # print(\"=\"*100)\n",
    "            # loss, _ = label_smoothed_nll_loss_gen(generated_tokens, labels, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "            # break\n",
    "            # loss, _ = map(label_smoothed_nll_loss, generated_tokens, labels, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "            # val_loss.extend(loss)\n",
    "\n",
    "            generated_tokens = accelerator.gather(\n",
    "                generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            if args.ignore_pad_token_for_loss:\n",
    "                # Replace -100 in the labels as we can't decode them.\n",
    "                labels = np.where(labels != -100, labels,\n",
    "                                tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "\n",
    "            # print(generated_tokens[0])\n",
    "            # print(\"=\"*100)\n",
    "            # print(labels[0])\n",
    "            # loss, _ = label_smoothed_nll_loss(generated_tokens[0], labels[0], args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "            # break\n",
    "\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(\n",
    "                labels, skip_special_tokens=True)\n",
    "\n",
    "            # print(decoded_preds)\n",
    "            # print(\"=\"*100)\n",
    "            # print(decoded_labels)\n",
    "            # print(\"=\"*100)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels)\n",
    "\n",
    "            # print(decoded_preds)\n",
    "            # print(\"=\"*100)\n",
    "            # print(decoded_labels)\n",
    "            # print(\"=\"*100)\n",
    "            # if step == 1:\n",
    "            #     break\n",
    "\n",
    "            val_predict.extend(decoded_preds)\n",
    "            val_groundtruth.extend(decoded_labels)\n",
    "\n",
    "    if args.len_output == 'real':\n",
    "        new_val_predict = []\n",
    "        for sample in val_predict:\n",
    "            try:\n",
    "                gen_sum = sample.split('Summary: ')[2]\n",
    "                new_val_predict.append(gen_sum)\n",
    "            except:\n",
    "                new_val_predict.append(sample)\n",
    "        val_predict = new_val_predict\n",
    "    else:\n",
    "        new_val_predict = val_predict\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Rouge score on val set after epoch {}\".format(epoch+1))\n",
    "    eval_results = py_rouge_scores(val_predict, val_groundtruth)\n",
    "\n",
    "    if best_r2_f1 is None:\n",
    "        best_r2_f1 = eval_results\n",
    "    if eval_results['rouge-2']['f'] >= best_r2_f1['rouge-2']['f']:\n",
    "        best_r2_f1 = eval_results\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "        os.makedirs(args.output_dir+'/best', exist_ok=True)\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            args.output_dir+'/best', save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir+'/best')\n",
    "        # save vocab\n",
    "        vocab = tokenizer.vocab.copy()\n",
    "        vocab = {k: v for k, v in sorted(\n",
    "            vocab.items(), key=lambda item: item[1])}\n",
    "        with open(args.output_dir + '/best/vocab.txt', 'w') as f:\n",
    "            for word, index in vocab.items():\n",
    "                # it lead to encoding bug on some machines, so i add this line\n",
    "                word = word.encode('ascii', 'ignore').decode('ascii')\n",
    "                f.write(str(index) + ': ' + word + '\\n')\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    logger.info(\n",
    "        \"Current Best Validation Result is at epoch {}\".format(best_epoch))\n",
    "    py_rouge_scores(None, None, best_r2_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTRdH7I6su3a",
    "outputId": "1bdae418-e213-4f73-a779-72e61fde2b04",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.encoder_last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0Ckx5K5su3b",
    "outputId": "6a0602b6-9e3d-442e-e3d9-486bb7f379a6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0199,  0.0383,  0.0413,  ..., -0.0192,  0.0033, -0.0034],\n",
       "         [-0.1682, -0.0113,  0.1301,  ..., -0.0883, -0.0093, -0.1615],\n",
       "         [-0.0111, -0.4472,  0.0442,  ...,  0.0226, -0.0170, -0.2872],\n",
       "         ...,\n",
       "         [-0.0308, -0.2955, -0.3913,  ...,  0.0104, -0.0009, -0.0810],\n",
       "         [-0.0451, -0.4547, -0.5451,  ...,  0.1203, -0.0126,  0.1627],\n",
       "         [-0.0310, -0.0535, -0.3161,  ...,  0.0269, -0.0641,  0.1126]],\n",
       "\n",
       "        [[ 0.0211,  0.0319,  0.0484,  ..., -0.0260, -0.0024, -0.0104],\n",
       "         [-0.2227, -0.0779, -0.0718,  ...,  0.0067, -0.0177, -0.0376],\n",
       "         [-0.1187, -0.5692, -0.0248,  ..., -0.0780,  0.0284, -0.4738],\n",
       "         ...,\n",
       "         [-0.0440,  0.1978, -0.3247,  ..., -0.0017,  0.1312,  0.3230],\n",
       "         [-0.1271, -0.3494, -0.5145,  ...,  0.0051,  0.1007,  0.0965],\n",
       "         [-0.1246, -0.2684, -0.6091,  ...,  0.0990,  0.0489, -0.0175]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.encoder_last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fko7L0FCsu3c",
    "outputId": "31ce65fa-1a28-40a1-fcd3-917c401f3a3d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.encoder_last_hidden_state.size(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNyUX1MBsu3c",
    "outputId": "a4828cfc-fcea-454f-859f-93b32feb3ad1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([48, 50266])\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs.logits))\n",
    "print(outputs.logits[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNn3RQYUsu3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_loss = torch.nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_VytNi5su3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_cs = cosine_loss(outputs.encoder_last_hidden_state[0], outputs.encoder_last_hidden_state[1], torch.ones(outputs.encoder_last_hidden_state.size(dim=1)).to(torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBFlUpC5su3d",
    "outputId": "7192015b-7569-4fed-8a9c-79d4b2833650",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7458, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "Mh3MeO5hsu3d",
    "outputId": "9500b993-deb4-49f7-fdae-089ecb787d16",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/04/2023 19:07:23 - INFO - __main__ - Loading Best Result is at epoch 4 for Testing\n",
      "loading configuration file ./output/1-bart-baseline-loss/best/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 1,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 1,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50266\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading weights file ./output/1-bart-baseline-loss/best/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./output/1-bart-baseline-loss/best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./output/1-bart-baseline-loss/best/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "10/04/2023 19:07:29 - INFO - __main__ - Collecting Testing Result...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "10/04/2023 19:08:05 - INFO - __main__ - \n",
      "10/04/2023 19:08:05 - INFO - __main__ - ROUGE score on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic of Summary: communication method. Length of Summary: 27. Dialogue: # Person1 # : Ms. Dawson , I need you to take a dictation for me . # Person2 # : Yes , sir ... # Person1 # : This should go out as an intra-office memorandum to all employees by this afternoon . Are you ready ? # Person2 # : Yes , sir . Go ahead . # Person1 # : Attention all staff ... Effective immediately , all office [TAG]communications[TAG] are restricted to email correspondence and official memos . The use of Instant Message programs by employees during working hours is strictly prohibited . # Person2 # : Sir , does this apply to intra-office [TAG]communications[TAG] only ? Or will it also restrict external [TAG]communications[TAG] ? # Person1 # : It should apply to all [TAG]communications[TAG] , not only in this office between employees , but also any outside [TAG]communications[TAG] . # Person2 # : But sir , many employees use Instant Messaging to communicate with their clients . # Person1 # : They will just have to change their [TAG]communication[TAG] [TAG]methods[TAG] . I do n't want any - one using Instant Messaging in this office . It wastes too much time ! Now , please continue with the memo . Where were we ? # Person2 # : This applies to internal and external [TAG]communications[TAG] . # Person1 # : Yes . Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation . At second offense , the employee will face termination . Any questions regarding this new policy may be directed to department heads . # Person2 # : Is that all ? # Person1 # : Yes . Please get this memo typed up and distributed to all employees before 4 pm .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/04/2023 19:08:05 - INFO - root - \n",
      "10/04/2023 19:08:05 - INFO - root - \trouge-1:\tP: 37.12\tR: 47.88\tF1: 40.73\n",
      "10/04/2023 19:08:05 - INFO - root - \trouge-2:\tP: 13.30\tR: 17.25\tF1: 14.64\n",
      "10/04/2023 19:08:05 - INFO - root - \trouge-l:\tP: 35.01\tR: 43.23\tF1: 37.96\n",
      "10/04/2023 19:08:05 - INFO - root - \n",
      "10/04/2023 19:08:05 - INFO - __main__ - \n"
     ]
    }
   ],
   "source": [
    "# =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = Test =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =\n",
    "# load best model\n",
    "logger.info(\"Loading Best Result is at epoch {} for Testing\".format(best_epoch))\n",
    "\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "config          = config.from_pretrained(args.output_dir+'/best')\n",
    "tokenizer       = tokenizer.from_pretrained(args.output_dir+'/best', config=config)\n",
    "unwrapped_model = unwrapped_model.from_pretrained(args.output_dir+'/best', config=config)\n",
    "model           = accelerator.prepare(unwrapped_model)\n",
    "\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError('{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "# start Test\n",
    "logger.info(\"Collecting Testing Result...\")\n",
    "model.eval()\n",
    "\n",
    "test_predict     = []\n",
    "test_groundtruth = []\n",
    "for step, batch in enumerate(tqdm(test_dataloader, leave=False)):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        if not args.pad_to_max_length:\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "        generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "        labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "        if args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "\n",
    "        decoded_preds  = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        decoded_preds  = [' '.join(sent.split('\\n')) for sent in decoded_preds]\n",
    "        decoded_labels = [' '.join(sent.split('\\n')) for sent in decoded_labels]\n",
    "\n",
    "        test_predict.extend(decoded_preds)\n",
    "        test_groundtruth.extend(decoded_labels)\n",
    "\n",
    "print(raw_datasets['test']['dialogue'][0])\n",
    "\n",
    "if args.len_output == 'real':\n",
    "    new_test_predict = []\n",
    "    for sample in test_predict:\n",
    "        try:\n",
    "            gen_sum = sample.split('Summary: ')[2]\n",
    "            new_test_predict.append(gen_sum)\n",
    "        except:\n",
    "            new_test_predict.append(sample)\n",
    "    test_predict = new_test_predict\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"ROUGE score on test set\")\n",
    "test_scores = py_rouge_scores(test_predict, test_groundtruth)\n",
    "logger.info(\"\")\n",
    "\n",
    "\n",
    "# Save generated summaries\n",
    "if args.len_input == 'predict':\n",
    "    os.makedirs(args.output_dir+'/predict_gen_samples', exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(args.output_dir+'/gen_samples', exist_ok=True)\n",
    "\n",
    "for i in range(len(test_predict)):\n",
    "    test_id        = raw_datasets['test']['id'][i]\n",
    "    test_dialogue  = raw_datasets['test']['dialogue'][i]\n",
    "    test_summary   = raw_datasets['test']['summary'][i]\n",
    "    test_predict_s = test_predict[i]\n",
    "\n",
    "    if args.len_input == 'predict':\n",
    "        with open(args.output_dir+'/predict_gen_samples/'+str(test_id)+'.txt', 'w') as f:\n",
    "            test_dialogue = test_dialogue.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_dialogue)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Golden Summary:\\n')\n",
    "            test_summary = test_summary.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_summary)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Generate Summary:\\n')\n",
    "            test_predict_s = test_predict_s.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_predict_s)\n",
    "    else:\n",
    "        with open(args.output_dir+'/gen_samples/'+str(test_id)+'.txt', 'w') as f:\n",
    "            test_dialogue = test_dialogue.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_dialogue)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Golden Summary:\\n')\n",
    "            test_summary = test_summary.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_summary)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Generate Summary:\\n')\n",
    "            test_predict_s = test_predict_s.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_predict_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvNEVEiCsu3e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuPFvbUWsu3e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13bb8c4b9a154bb99c4d38cd2edb631c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14052c5abfad4231a0d4c8d0e0395a01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "24272444846441638d4836fd835562dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78f6c1bca77640ca92bd87d5995fced5",
       "IPY_MODEL_81669ba2443a489cb535c508333dc974",
       "IPY_MODEL_47ddb2c2d8fa4c278c9d514ace68341f"
      ],
      "layout": "IPY_MODEL_b842e08bc1ca45ee99a868d1dfa4d0db"
     }
    },
    "31409e259c8b4ee7899035ea4ba3f140": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "326356876bd540778d188c41673e04f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35f50d433dc14aa28cf0f5a039f7cb87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "368a820467ad4f4d8198932125f0d357": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bad8094e9124ab391d945edf4ccca2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4616558f0ebd466babad0202bc55824d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47ddb2c2d8fa4c278c9d514ace68341f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_932e4629e7f340c2a5b64d095c46bf4d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a26c7dc74d71494787fd32d560594b38",
      "value": " 500/500 [00:01&lt;00:00, 317.13 examples/s]"
     }
    },
    "542a00a050384f5f8f3f7354eb527336": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a2947306125452c8c25c12e54dc6656": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78f6c1bca77640ca92bd87d5995fced5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4616558f0ebd466babad0202bc55824d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9a4ed651601f4a29b763eed10afafc16",
      "value": "Running tokenizer on dataset: 100%"
     }
    },
    "7f1272db213a4121ae95b5e41061d964": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e69f5057741b46dba3ebb186ef85c6db",
      "max": 1500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14052c5abfad4231a0d4c8d0e0395a01",
      "value": 1500
     }
    },
    "81669ba2443a489cb535c508333dc974": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_368a820467ad4f4d8198932125f0d357",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f5ece15e08e44905a09d9a2e988a4be4",
      "value": 500
     }
    },
    "852ba2b2a4fd4bbe80a2ecd9c9f6b5fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1a15a93c5e746888806f8dd19cf6889",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_35f50d433dc14aa28cf0f5a039f7cb87",
      "value": "Running tokenizer on dataset: 100%"
     }
    },
    "90b25ecb208546f9938652c360ef7a5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_852ba2b2a4fd4bbe80a2ecd9c9f6b5fb",
       "IPY_MODEL_7f1272db213a4121ae95b5e41061d964",
       "IPY_MODEL_a3ad3e93702041d89b6f505ec6d37411"
      ],
      "layout": "IPY_MODEL_ffcb842d99954cdcaa8fb78e8e3b2215"
     }
    },
    "932e4629e7f340c2a5b64d095c46bf4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a4ed651601f4a29b763eed10afafc16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c182b108efa471994186923f10871a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a26c7dc74d71494787fd32d560594b38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3ad3e93702041d89b6f505ec6d37411": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bad8094e9124ab391d945edf4ccca2f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_31409e259c8b4ee7899035ea4ba3f140",
      "value": " 1500/1500 [00:03&lt;00:00, 414.56 examples/s]"
     }
    },
    "a91a8c5870234ab3bdd0132addb34a20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1a15a93c5e746888806f8dd19cf6889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b842e08bc1ca45ee99a868d1dfa4d0db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c34934bec45646a8b2ac76c60dc124d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0098c03b82d48c9acdc684fbbf54567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa32d34f467944f2aaf4a13fdf7a0cf2",
       "IPY_MODEL_e3bc145889c94fc3974e88e46863db89",
       "IPY_MODEL_ddbc6421b4cf487d92cc53fb0ae6f634"
      ],
      "layout": "IPY_MODEL_13bb8c4b9a154bb99c4d38cd2edb631c"
     }
    },
    "ddbc6421b4cf487d92cc53fb0ae6f634": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c182b108efa471994186923f10871a8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c34934bec45646a8b2ac76c60dc124d5",
      "value": " 12460/12460 [00:20&lt;00:00, 564.66 examples/s]"
     }
    },
    "e3bc145889c94fc3974e88e46863db89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_542a00a050384f5f8f3f7354eb527336",
      "max": 12460,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a91a8c5870234ab3bdd0132addb34a20",
      "value": 12460
     }
    },
    "e69f5057741b46dba3ebb186ef85c6db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5ece15e08e44905a09d9a2e988a4be4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fa32d34f467944f2aaf4a13fdf7a0cf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a2947306125452c8c25c12e54dc6656",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_326356876bd540778d188c41673e04f5",
      "value": "Running tokenizer on dataset: 100%"
     }
    },
    "ffcb842d99954cdcaa8fb78e8e3b2215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
