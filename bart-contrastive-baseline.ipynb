{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()  # Initiate nltk lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_tokenize(sentence):\n",
    "    \"\"\" Simple function for tokenizing text with nltk \"\"\"\n",
    "    return nltk.word_tokenize(sentence, preserve_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from filelock import FileLock\n",
    "from transformers import AdamW, get_scheduler, set_seed\n",
    "\n",
    "from transformers.file_utils import is_offline_mode\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "# from args import parse_args\n",
    "# from data_loader import raw_data_loader, data_processor\n",
    "from model_loader import model_loader\n",
    "from rouge_s import py_rouge_scores\n",
    "from utils import label_smoothed_nll_loss, postprocess_text\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MODEL_MAPPING,\n",
    "    SchedulerType,\n",
    ")\n",
    "\n",
    "# You should update this to your particular problem to have better documentation of `model_type`\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--topic_tagger'], dest='topic_tagger', nargs=None, const=None, default=None, type=<class 'bool'>, choices=None, required=False, help='Use topic tag [TAG] or not', metavar=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "arg_parser = argparse.ArgumentParser(description=\"BART\")\n",
    "arg_parser.add_argument(\"--len_input\", dest=\"len_input\", type=str, default=None, help=\"set up prefix input\",choices=('no', 'topic', 'length', 'topic-length', 'length-topic', 'simple', 'simple-topic-tagger', 'simple-tagger'))\n",
    "arg_parser.add_argument(\"--len_output\", dest=\"len_output\", default=None, help=\"Use the ctrlen model or not\", choices=('no', 'topic', 'length', 'topic-length', 'length-topic'))\n",
    "arg_parser.add_argument(\"--output_dir\", dest=\"output_dir\", type=str, default=\"./output/1\", help=\"default\")\n",
    "arg_parser.add_argument(\"--train_file\", dest=\"train_file\", type=str, default=None, help=\"A csv or a json file containing the training data.\")\n",
    "arg_parser.add_argument(\"--validation_file\", dest=\"validation_file\", type=str, default=None, help=\"A csv or a json file containing the validation data.\")\n",
    "arg_parser.add_argument(\"--test_file\", dest=\"test_file\", type=str, default=None, help=\"A csv or a json file containing the test data.\")\n",
    "arg_parser.add_argument(\"--ignore_pad_token_for_loss\", dest=\"ignore_pad_token_for_loss\", type=bool, default=True, help=\"Whether to ignore the tokens corresponding to \" \"padded labels in the loss computation or not.\",)\n",
    "arg_parser.add_argument(\"--text_column\", dest=\"text_column\", type=str, default=\"dialogue\", help=\"The name of the column in the datasets containing the full texts (for summarization).\")\n",
    "arg_parser.add_argument(\"--summary_column\", dest=\"summary_column\", type=str, default=\"summary\", help=\"The name of the column in the datasets containing the summaries (for summarization).\")\n",
    "arg_parser.add_argument(\"--model_name_or_path\", dest=\"model_name_or_path\", type=str, default=\"facebook/bart-large\", help=\"Path to pretrained model or model identifier from huggingface.co/models.\")\n",
    "arg_parser.add_argument(\"--model_type\", dest=\"model_type\", type=str, default=\"bart\", help=\"Model type to use if training from scratch.\", choices=MODEL_TYPES)\n",
    "arg_parser.add_argument(\"--max_source_length\", dest=\"max_source_length\", type=int, default=1024, help=\"default\")\n",
    "arg_parser.add_argument(\"--source_prefix\", dest=\"source_prefix\", type=str, default=None, help=\"A prefix to add before every source text \" \"(useful for T5 models).\")\n",
    "arg_parser.add_argument(\"--preprocessing_num_workers\", type=int, default=None, help=\"The number of processes to use for the preprocessing.\")\n",
    "# arg_parser.add_argument(\"--overwrite_cache\", dest=\"overwrite_cache\", type=lambda x:bool(strtobool(x)), default=True, help=\"default\")\n",
    "arg_parser.add_argument(\"--overwrite_cache\", dest=\"overwrite_cache\", type=bool, default=None, help=\"Overwrite the cached training and evaluation sets\")\n",
    "arg_parser.add_argument(\"--min_target_length\", dest=\"min_target_length\", type=int, default=1, help=\"The minimal total sequence length for target text\")\n",
    "arg_parser.add_argument(\"--max_target_length\", dest=\"max_target_length\", type=int, default=128, help=\"The maximum total sequence length for target text after \"\n",
    "        \"tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "        \"during ``evaluate`` and ``predict``.\")\n",
    "arg_parser.add_argument(\"--num_beams\", dest=\"num_beams\", type=int, default=4, help=\"Number of beams to use for evaluation. This argument will be \"\n",
    "        \"passed to ``model.generate``, which is used during ``evaluate`` and ``predict``.\")\n",
    "arg_parser.add_argument(\"--learning_rate\", dest=\"learning_rate\", type=float, default=5e-5, help=\"Initial learning rate (after the potential warmup period) to use.\")\n",
    "arg_parser.add_argument(\"--pad_to_max_length\", action=\"store_true\", help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.\",)\n",
    "arg_parser.add_argument(\"--weight_decay\", dest=\"weight_decay\", type=float, default=1e-3, help=\"Weight decay to use.\")\n",
    "arg_parser.add_argument(\"--label_smoothing\", dest=\"label_smoothing\", type=float, default=0.1, help=\"hyperparameter for label smoothing.\")\n",
    "arg_parser.add_argument(\"--length_penalty\", dest=\"length_penalty\", type=float, default=1.0, help=\"large - longer sequence, small - shorter sequence\")\n",
    "arg_parser.add_argument(\"--num_train_epochs\", dest=\"num_train_epochs\", type=int, default=15, help=\"Total number of training epochs to perform.\")\n",
    "arg_parser.add_argument(\"--per_device_train_batch_size\", dest=\"per_device_train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\")\n",
    "arg_parser.add_argument(\"--gradient_accumulation_steps\", dest=\"gradient_accumulation_steps\", type=int, default=64, help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "arg_parser.add_argument(\"--per_device_eval_batch_size\", dest=\"per_device_eval_batch_size\", type=int, default=8, help=\"Batch size (per device) for the evaluation dataloader.\")\n",
    "arg_parser.add_argument(\"--per_device_test_batch_size\", dest=\"per_device_test_batch_size\", type=int, default=8, help=\"Batch size (per device) for the evaluation dataloader.\")\n",
    "arg_parser.add_argument(\"--num_warmup_steps\", dest=\"num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\")\n",
    "arg_parser.add_argument(\"--cache_dir\", dest=\"cache_dir\", type=str, default=\"./output/cache\", help=\"default\")\n",
    "arg_parser.add_argument(\"--seed\", dest=\"seed\", type=int, default=12345, help=\"default\")\n",
    "# arg_parser.add_argument(\"-f\", required=False) #important\n",
    "arg_parser.add_argument(\"--config_name\", type=str, default=None, help=\"Pretrained config name or path if not the same as model_name\")\n",
    "arg_parser.add_argument(\"--tokenizer_name\", type=str, default=None, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "arg_parser.add_argument(\"--use_slow_tokenizer\", dest=\"use_slow_tokenizer\", action=\"store_true\", help=\"If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).\")\n",
    "arg_parser.add_argument(\"--max_train_steps\", type=int, default=None, help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\")\n",
    "arg_parser.add_argument(\"--lr_scheduler_type\", type=SchedulerType, default=\"linear\", help=\"The scheduler type to use.\", choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"])\n",
    "arg_parser.add_argument(\"--ctrlen_model\", action='store_true', default=False, help=\"Use the ctrlen model or not\")\n",
    "arg_parser.add_argument(\"--sim_window_size\", type=int, default=5, help=\"window size for computing loss.\")\n",
    "arg_parser.add_argument(\"--sim_loss\", type=float, default=0, help=\"the loss weight for similarity scores.\")\n",
    "arg_parser.add_argument(\"--special_len_token_init\", type=str, default=None, help=\"ways to initialize special token for length (random, zero, token_embs)\")\n",
    "arg_parser.add_argument(\"--embedding_lr\", type=float, default=5e-5, help=\"Initial learning rate for embedding layers.\")\n",
    "arg_parser.add_argument(\"--len_start\", type=int, default=1, help=\"start length.\")\n",
    "arg_parser.add_argument(\"--len_end\", type=int, default=100, help=\"end length.\")\n",
    "arg_parser.add_argument(\"--data_aug\",action='store_true',default=False,help=\"whether to perform data augmentation or not\")\n",
    "arg_parser.add_argument(\"--pred_len\", action='store_true', default=False, help=\"whether to use the golden length or predicted length\")\n",
    "arg_parser.add_argument(\"--shuffle\", action='store_true', default=False, help=\"whether to shuffle the dataset to balance train/validation/test\")\n",
    "arg_parser.add_argument(\"--debug\", action='store_true', default=False, help=\"Use the debug mode or not\")\n",
    "\n",
    "arg_parser.add_argument(\"--topic_tagger\", dest=\"topic_tagger\", type=bool, default=None, help=\"Use topic tag [TAG] or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = arg_parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.train_file = \"./data/dialogtest/dialogsum.train.jsonl\"\n",
    "args.validation_file = \"./data/dialogtest/dialogsum.dev.jsonl\"\n",
    "args.test_file = \"./data/dialogtest/dialogsum.test.jsonl\"\n",
    "args.text_column = \"dialogue\"\n",
    "args.summary_column = \"summary\"\n",
    "args.model_name_or_path = \"facebook/bart-large\"\n",
    "args.model_type = \"bart\"\n",
    "args.max_source_length = 1024\n",
    "args.min_target_length = 1\n",
    "args.max_target_length = 128\n",
    "args.num_beams = 4\n",
    "args.learning_rate = 5e-5\n",
    "args.weight_decay = 1e-3\n",
    "args.label_smoothing = 0.1\n",
    "args.length_penalty = 1.0 \n",
    "args.num_train_epochs = 1\n",
    "args.per_device_train_batch_size = 2 \n",
    "args.gradient_accumulation_steps = 64 \n",
    "args.per_device_eval_batch_size = 8 \n",
    "args.per_device_test_batch_size = 8 \n",
    "args.num_warmup_steps = 0 \n",
    "args.cache_dir = \"./output/cache\"\n",
    "args.overwrite_cache = True\n",
    "args.seed = 12345\n",
    "\n",
    "args.len_input = 'topic-length'\n",
    "args.len_output = 'no'\n",
    "args.output_dir = \"./output/1-bart-baseline-loss\"\n",
    "\n",
    "args.topic_tagger = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic-length\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(args.len_input)\n",
    "print(args.topic_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_tokenize(sentence):\n",
    "    \"\"\" Simple function for tokenizing text with nltk \"\"\"\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def nltk_to_pos(pos):\n",
    "    \"\"\" Simple function for converting nltk pos to wordnet pos\"\"\"\n",
    "    if pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\" Function to lemmatize text according to the wordnet POS of each token \"\"\"\n",
    "\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    POS_assigned_text = nltk.pos_tag(tokenized_text)\n",
    "\n",
    "    available_POS = map(lambda x: (x[0], nltk_to_pos(x[1])), POS_assigned_text)\n",
    "\n",
    "    lemmatized_text = [token if pos is None\n",
    "                       else lemmatizer.lemmatize(token, pos)\n",
    "                       for token, pos in available_POS]\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "def build_tagger(original_tokens,lemmatized_tokens, topic_list, idx):\n",
    "    tagged_tokens = []\n",
    "    # Extract all the seed words according to the corresponding topic\n",
    "    token_topics = topic_list\n",
    "    original_list = original_tokens[idx]\n",
    "\n",
    "    for j, token in enumerate(lemmatized_tokens[idx]):\n",
    "        # If the lemmatized form of the token is in topic seeds, tag the original token\n",
    "        if token.lower() in token_topics:\n",
    "            # print(token.lower())\n",
    "            if token.lower() not in stopwords.words('english'):\n",
    "                # print(\"=\"*100)\n",
    "                # print(token.lower())\n",
    "                original_list[j] = '[TAG]' + original_list[j] + '[TAG]'\n",
    "\n",
    "    tagged_tokens.append(\" \".join(original_list))\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_from_dialogsum(args, file_path):\n",
    "    ''' load dialoguesum jsonl data '''\n",
    "\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    id_list = [sample['fname'] for sample in data]\n",
    "    dialogue_list = [sample['dialogue'] for sample in data]\n",
    "\n",
    "    if 'summary' in data[0]:\n",
    "        # summary\n",
    "        summary_list = [sample['summary'] for sample in data]\n",
    "        # topic\n",
    "        topic_list = [sample['topic'] for sample in data]\n",
    "\n",
    "    elif 'summary1' in data[0]:\n",
    "\n",
    "        id_list1 = [id+\"_sum1\" for id in id_list]\n",
    "        id_list2 = [id+\"_sum2\" for id in id_list]\n",
    "        id_list3 = [id+\"_sum3\" for id in id_list]\n",
    "\n",
    "        id_list = id_list1 + id_list2 + id_list3\n",
    "        dialogue_list = dialogue_list + dialogue_list + dialogue_list\n",
    "\n",
    "        # summary\n",
    "        summary_list1 = [sample['summary1'] for sample in data]\n",
    "        summary_list2 = [sample['summary2'] for sample in data]\n",
    "        summary_list3 = [sample['summary3'] for sample in data]\n",
    "\n",
    "        summary_list = summary_list1 + summary_list2 + summary_list3\n",
    "\n",
    "        # topic\n",
    "        topic_list1 = [sample['topic1'] for sample in data]\n",
    "        topic_list2 = [sample['topic2'] for sample in data]\n",
    "        topic_list3 = [sample['topic3'] for sample in data]\n",
    "\n",
    "        topic_list = topic_list1 + topic_list2 + topic_list3\n",
    "        \n",
    "    negative_topic_list = []\n",
    "    for topic in topic_list:\n",
    "        negative_topic = random.choice(topic_list)\n",
    "        if negative_topic == topic:\n",
    "            negative_topic = random.choice(negative_topic)\n",
    "        negative_topic_list.append(negative_topic)\n",
    "        \n",
    "\n",
    "    if args.topic_tagger:\n",
    "        topic_tagger = []\n",
    "        original_tokens = [simple_tokenize(x) for x in dialogue_list]\n",
    "        lemmatized_tokens = [lemmatize_text(x) for x in dialogue_list]\n",
    "        for i in range(len(lemmatized_tokens)):\n",
    "            tagger = build_tagger(original_tokens, lemmatized_tokens, topic_list[i], i)\n",
    "            topic_tagger.extend(tagger)\n",
    "\n",
    "        data_dict = {'id': id_list,\n",
    "                     'dialogue': topic_tagger,\n",
    "                     'summary': summary_list,\n",
    "                     'topic': topic_list}\n",
    "    else:\n",
    "        data_dict = {'id': id_list,\n",
    "                     'dialogue': dialogue_list,\n",
    "                     'summary': summary_list,\n",
    "                     'topic': topic_list,\n",
    "                     'negative_topic': negative_topic_list}\n",
    "\n",
    "    data_dict = Dataset.from_dict(data_dict)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dict = load_from_dialogsum(args, args.train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'topic', 'negative_topic'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do a favor'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['topic'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'communication habits'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['negative_topic'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all(train_dict['topic'][i] != train_dict['negative_topic'][i] for i in range(len(train_dict['topic'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2023 19:12:54 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "logger.info(accelerator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config, tokenizer, model = model_loader(accelerator, logger, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "accelerator.is_local_main_process\n",
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "set_seed(args.seed)\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "accelerator.is_main_process\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def raw_data_loader(args):\n",
    "    ''' load raw datasets from csv files '''\n",
    "\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    if args.test_file is not None:\n",
    "        data_files[\"test\"] = args.test_file\n",
    "\n",
    "    if 'dialogsum' in args.train_file:\n",
    "        train_dict = load_from_dialogsum(args, args.train_file)\n",
    "        val_dict   = load_from_dialogsum(args, args.validation_file)\n",
    "        test_dict  = load_from_dialogsum(args, args.test_file)\n",
    "\n",
    "    train_dict = utils.len_adjust(args, train_dict, 'train')\n",
    "    val_dict   = utils.len_adjust(args, val_dict, 'val')\n",
    "    test_dict  = utils.len_adjust(args, test_dict, 'test')\n",
    "\n",
    "    raw_datasets = datasets.DatasetDict({\"train\":train_dict, \"validation\":val_dict, \"test\":test_dict})\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_datasets = raw_data_loader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'negative_dialogue', 'summary', 'topic'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Topic of Summary: vaccines. Length of Summary: 18. Dialogue: #Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['dialogue'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Topic of Summary: get dressed. Length of Summary: 18. Dialogue: #Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['negative_dialogue'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomDataCollator:\n",
    "#     def __init__(self, tokenizer, model):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.model = model\n",
    "\n",
    "#     def __call__(self, examples):\n",
    "#         # positive_input_ids = examples['dialogue']\n",
    "#         positive_input = [np.array(example['dialogue']) for example in examples]\n",
    "#         # positive_input_ids = [example['dialogue'][example['dialogue'] != 1] for example in examples]\n",
    "#         # negative_input_ids = examples['negative_dialogue']\n",
    "#         negative_input = [np.array(example['negative_dialogue']) for example in examples]\n",
    "#         # negative_input_ids = [example['negative_dialogue'][example['dialogue'] != 1] for example in examples]\n",
    "#         # summary_input_ids = examples['summary']\n",
    "#         summary_input = [np.array(example['summary']) for example in examples]\n",
    "#         # summary_input_ids = [example['summary'][example['dialogue'] != 1] for example in examples]\n",
    "\n",
    "#         positive_input_ids = [example[example != 1] for example in positive_input]\n",
    "#         negative_input_ids = [example[example != 1] for example in negative_input]\n",
    "#         summary_input_ids = [example[example != 1] for example in summary_input]\n",
    "#         # summary_input_ids = [example['summary'] for example in examples]\n",
    "            \n",
    "#         # inputs[\"input_ids\"] = tokenizer.pad\n",
    "#         # negative_inputs[\"input_ids\"] = tokenizer.pad\n",
    "        \n",
    "#         batch = self.tokenizer.pad(encoded_inputs={\"input_ids\": positive_input_ids+ negative_input_ids}, padding=True, return_tensors='pt')\n",
    "#         # batch[\"decoder_input_ids\"] = torch.stack((inputs[\"labels\"], inputs[\"labels\"]))\n",
    "#         # batch[\"decoder_attention_mask\"] = torch.stack((inputs[\"labels\"], inputs[\"decoder_attention_mask\"]))\n",
    "#         batch[\"decoder_input_ids\"] = self.tokenizer.pad(encoded_inputs={\"input_ids\": summary_input_ids+summary_input_ids}, padding=True, return_tensors='pt')[\"input_ids\"]\n",
    "#         # summary = [[(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in summary_input_ids]\n",
    "#         summary = self.tokenizer.pad(encoded_inputs={\"input_ids\": summary_input_ids+summary_input_ids}, padding=True, return_tensors='pt')[\"input_ids\"]\n",
    "#         summary[summary == 1] = -100\n",
    "#         batch[\"labels\"] = summary\n",
    "#         # summary =  self.tokenizer.pad(encoded_inputs={\"input_ids\": summary_input_ids+summary_input_ids}, padding=True, return_tensors='pt')[\"input_ids\"]\n",
    "#         # summary = [[(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in summary_input_ids]\n",
    "\n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataCollatorValidate:\n",
    "#     def __init__(self, tokenizer, model):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.model = model\n",
    "\n",
    "#     def __call__(self, examples):\n",
    "#         # positive_input_ids = examples['dialogue']\n",
    "#         positive_input = [np.array(example['dialogue']) for example in examples]\n",
    "#         # positive_input_ids = [example['dialogue'][example['dialogue'] != 1] for example in examples]\n",
    "#         # negative_input_ids = examples['negative_dialogue']\n",
    "#         # negative_input = [np.array(example['negative_dialogue']) for example in examples]\n",
    "#         # negative_input_ids = [example['negative_dialogue'][example['dialogue'] != 1] for example in examples]\n",
    "#         # summary_input_ids = examples['summary']\n",
    "#         summary_input = [np.array(example['summary']) for example in examples]\n",
    "#         # summary_input_ids = [example['summary'][example['dialogue'] != 1] for example in examples]\n",
    "\n",
    "#         positive_input_ids = [example[example != 1] for example in positive_input]\n",
    "#         # negative_input_ids = [example[example != 1] for example in negative_input]\n",
    "#         summary_input_ids = [example[example != 1] for example in summary_input]\n",
    "#         # summary_input_ids = [example['summary'] for example in examples]\n",
    "            \n",
    "#         # inputs[\"input_ids\"] = tokenizer.pad\n",
    "#         # negative_inputs[\"input_ids\"] = tokenizer.pad\n",
    "        \n",
    "#         batch = self.tokenizer.pad(encoded_inputs={\"input_ids\": positive_input_ids}, padding=True, return_tensors='pt')\n",
    "#         # batch[\"decoder_input_ids\"] = torch.stack((inputs[\"labels\"], inputs[\"labels\"]))\n",
    "#         # batch[\"decoder_attention_mask\"] = torch.stack((inputs[\"labels\"], inputs[\"decoder_attention_mask\"]))\n",
    "#         batch[\"decoder_input_ids\"] = self.tokenizer.pad(encoded_inputs={\"input_ids\": summary_input_ids}, padding=True, return_tensors='pt')[\"input_ids\"]\n",
    "#         # summary = [[(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in summary_input_ids]\n",
    "#         summary = self.tokenizer.pad(encoded_inputs={\"input_ids\": summary_input_ids}, padding=True, return_tensors='pt')[\"input_ids\"]\n",
    "#         summary[summary == 1] = -100\n",
    "#         batch[\"labels\"] = summary\n",
    "#         # summary =  self.tokenizer.pad(encoded_inputs={\"input_ids\": summary_input_ids+summary_input_ids}, padding=True, return_tensors='pt')[\"input_ids\"]\n",
    "#         # summary = [[(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in summary_input_ids]\n",
    "\n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def data_processor(logger, args, accelerator, raw_datasets, tokenizer, model):\n",
    "#     ''' prepare dataset format for train/val/test '''\n",
    "#     def preprocess_function(examples):\n",
    "#         positive_documents = examples['dialogue']\n",
    "#         negative_documents = examples['negative_dialogue']\n",
    "#         source_summaries = examples['summary']\n",
    "\n",
    "#         # Tokenize and create input tensors\n",
    "#         inputs = tokenizer(\n",
    "#             positive_documents,\n",
    "#             # negative_summaries,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=args.max_source_length  # Adjust as needed\n",
    "#         )\n",
    "        \n",
    "#         # Tokenize and create input tensors\n",
    "#         negative_inputs = tokenizer(\n",
    "#             negative_documents,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=args.max_source_length  # Adjust as needed\n",
    "#         )\n",
    "        \n",
    "#         with tokenizer.as_target_tokenizer():\n",
    "#             labels = tokenizer(\n",
    "#                 source_summaries,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 padding=True,\n",
    "#                 truncation=True,\n",
    "#                 max_length=args.max_target_length  # Adjust as needed\n",
    "#             )\n",
    "        \n",
    "#         model_inputs = inputs\n",
    "#         model_inputs[\"dialogue\"] = inputs[\"input_ids\"]\n",
    "#         model_inputs[\"negative_dialogue\"] = negative_inputs[\"input_ids\"]\n",
    "#         model_inputs[\"summary\"] = labels[\"input_ids\"]\n",
    "\n",
    "#         return model_inputs\n",
    "\n",
    "#     prefix = args.source_prefix if args.source_prefix is not None else \"\"\n",
    "\n",
    "#     # Preprocessing the datasets.\n",
    "#     # First we tokenize all the texts.\n",
    "#     column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "#     # Get the column names for input/target.\n",
    "#     text_column = args.text_column\n",
    "#     if text_column not in column_names:\n",
    "#         raise ValueError(\n",
    "#             f\"--text_column' value '{args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "#         )\n",
    "\n",
    "#     summary_column = args.summary_column\n",
    "#     if summary_column not in column_names:\n",
    "#         raise ValueError(\n",
    "#             f\"--summary_column' value '{args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "#         )\n",
    "\n",
    "#     # Temporarily set max_target_length for training.\n",
    "#     max_target_length = args.max_target_length\n",
    "#     padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "#     with accelerator.main_process_first():\n",
    "#         processed_datasets = raw_datasets.map(\n",
    "#             preprocess_function,\n",
    "#             batched=True,\n",
    "#             batch_size=100,\n",
    "#             remove_columns=column_names,\n",
    "#             load_from_cache_file=not args.overwrite_cache,\n",
    "#             desc=\"Running tokenizer on dataset\",\n",
    "#         )\n",
    "\n",
    "#     train_dataset = processed_datasets[\"train\"]\n",
    "#     eval_dataset  = processed_datasets[\"validation\"]\n",
    "#     test_dataset  = processed_datasets[\"test\"]\n",
    "\n",
    "#     # Log a few random samples from the training set:\n",
    "#     for index in random.sample(range(len(train_dataset)), 1):\n",
    "#         logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "#     label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "#     # data_collator = DataCollatorForSeq2Seq(\n",
    "#     #     tokenizer,\n",
    "#     #     model=model,\n",
    "#     #     label_pad_token_id=label_pad_token_id,\n",
    "#     #     pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    "#     # )\n",
    "#     data_collator = CustomDataCollator(\n",
    "#         tokenizer,\n",
    "#         model=model,\n",
    "#     )\n",
    "    \n",
    "#     validate_data_collator = CustomDataCollatorValidate(\n",
    "#         tokenizer,\n",
    "#         model=model,\n",
    "#     )\n",
    "\n",
    "#     train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n",
    "#     eval_dataloader = DataLoader(eval_dataset, collate_fn=validate_data_collator, batch_size=args.per_device_eval_batch_size)\n",
    "#     test_dataloader = DataLoader(test_dataset, collate_fn=validate_data_collator, batch_size=args.per_device_test_batch_size)\n",
    "\n",
    "#     return (train_dataloader, eval_dataloader, test_dataloader), (train_dataset, eval_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(logger, args, accelerator, raw_datasets, tokenizer, model):\n",
    "    ''' prepare dataset format for train/val/test '''\n",
    "    def preprocess_function(examples):\n",
    "\n",
    "        # summary - target\n",
    "        targets = examples[summary_column]\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "        if args.ctrlen_model:\n",
    "            gold_sum_len = [len(item) for item in labels['attention_mask']]\n",
    "\n",
    "        # dialogue - input\n",
    "        inputs = examples[text_column]\n",
    "        new_inputs = []\n",
    "        for i, inp in enumerate(inputs):\n",
    "            if args.ctrlen_model:\n",
    "                if 'pred_len' in examples:\n",
    "                    new_inputs.append(prefix + \"<len_{}> \".format(examples['pred_len'][i]) + inp)\n",
    "\n",
    "                else:\n",
    "                    new_inputs.append(prefix + \"<len_{}> \".format(gold_sum_len[i]) + inp)\n",
    "            else:\n",
    "                new_inputs.append(prefix + inp)\n",
    "\n",
    "        inputs = new_inputs\n",
    "        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "        # padding in the loss.\n",
    "        if padding == \"max_length\" and args.ignore_pad_token_for_loss:\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "        if args.ctrlen_model:\n",
    "            model_inputs[\"gold_len\"] = gold_sum_len\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "    # Get the column names for input/target.\n",
    "    text_column = args.text_column\n",
    "    if text_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--text_column' value '{args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "    summary_column = args.summary_column\n",
    "    if summary_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--summary_column' value '{args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "    # Temporarily set max_target_length for training.\n",
    "    max_target_length = args.max_target_length\n",
    "    padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        processed_datasets = raw_datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset  = processed_datasets[\"validation\"]\n",
    "    test_dataset  = processed_datasets[\"test\"]\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 1):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.per_device_test_batch_size)\n",
    "\n",
    "    return (train_dataloader, eval_dataloader, test_dataloader), (train_dataset, eval_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\n",
    "#             raw_datasets['train']['dialogue'][:2],\n",
    "#             # negative_summaries,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=args.max_source_length  # Adjust as needed\n",
    "#         )\n",
    "# negative_inputs = tokenizer(\n",
    "#             raw_datasets['train']['dialogue'][2:4],\n",
    "#             # negative_summaries,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=args.max_source_length  # Adjust as needed\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['input_ids'].shape\n",
    "# # inputs['input_ids'].numpy()\n",
    "# model_inputs = inputs\n",
    "# negative_inputs['input_ids'].shape\n",
    "# model_inputs['negative_dialogue'] = negative_inputs['input_ids']\n",
    "# type(model_inputs['negative_dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples):\n",
    "#     positive_documents = examples['dialogue']\n",
    "#     negative_documents = examples['negative_dialogue']\n",
    "#     source_summaries = examples['summary']\n",
    "\n",
    "#     # Tokenize and create input tensors\n",
    "#     inputs = tokenizer(\n",
    "#         positive_documents,\n",
    "#         # negative_summaries,\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=args.max_source_length  # Adjust as needed\n",
    "#     )\n",
    "    \n",
    "#     # Tokenize and create input tensors\n",
    "#     negative_inputs = tokenizer(\n",
    "#         negative_documents,\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=args.max_source_length  # Adjust as needed\n",
    "#     )\n",
    "    \n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(\n",
    "#             source_summaries,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=args.max_target_length  # Adjust as needed\n",
    "#         )\n",
    "    \n",
    "#     # batch = tokenizer.pad(encoded_inputs={\"input_ids\": inputs[\"input_ids\"].squeeze().tolist() + negative_inputs[\"input_ids\"].squeeze().tolist()}, padding=True, return_tensors='pt')\n",
    "#     # # batch[\"decoder_input_ids\"] = torch.stack((inputs[\"labels\"], inputs[\"labels\"]))\n",
    "#     # # batch[\"decoder_attention_mask\"] = torch.stack((inputs[\"labels\"], inputs[\"decoder_attention_mask\"]))\n",
    "#     # batch[\"decoder_input_ids\"] = tokenizer.pad(encoded_inputs={\"input_ids\": labels[\"input_ids\"].squeeze().tolist()+labels[\"input_ids\"].squeeze().tolist()}, padding=False, return_tensors='pt')[\"input_ids\"]\n",
    "#     # labels[\"input_ids\"] = [[(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]]\n",
    "#     # batch[\"labels\"] =  tokenizer.pad(encoded_inputs={\"input_ids\": labels[\"input_ids\"]+labels[\"input_ids\"]}, padding=True, return_tensors='pt')[\"input_ids\"]\n",
    "\n",
    "#     # return batch\n",
    "#     model_inputs = inputs\n",
    "#     model_inputs[\"dialogue\"] = inputs[\"input_ids\"]\n",
    "#     model_inputs[\"negative_dialogue\"] = negative_inputs[\"input_ids\"]\n",
    "#     model_inputs[\"summary\"] = labels[\"input_ids\"]\n",
    "\n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with accelerator.main_process_first():\n",
    "#     processed_datasets = raw_datasets.map(\n",
    "#         preprocess_function,\n",
    "#         batched=True,\n",
    "#         batch_size=1000,\n",
    "#         # remove_columns=column_names,\n",
    "#         load_from_cache_file=not args.overwrite_cache,\n",
    "#         desc=\"Running tokenizer on dataset\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(processed_datasets['train']['dialogue'][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example in processed_datasets['train']:\n",
    "#     dialogue = example['dialogue']\n",
    "#     print(type(dialogue))\n",
    "#     # print([i for i in dialogue if i != 1])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_input = [example for example in processed_datasets['train']['dialogue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe605a855b74622ad4b0adaedccf7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b010741d48a145a3b6131f6d22bc9594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5630a5ad4894de8895472e3a4474f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2023 19:13:14 - INFO - __main__ - Sample 580 of the training set: {'input_ids': [0, 48931, 9, 19584, 35, 8018, 4, 41852, 9, 19584, 35, 706, 4, 33854, 35, 849, 41761, 134, 10431, 35, 653, 18, 70, 14, 11347, 59, 116, 50118, 10431, 41761, 176, 10431, 35, 38, 95, 13414, 103, 2480, 514, 15, 6918, 6, 150, 79, 21, 11, 5, 2131, 9310, 6, 47, 197, 33, 450, 69, 652, 4, 50118, 10431, 41761, 134, 10431, 35, 370, 4395, 75, 29993, 110, 2761, 98, 203, 4, 50118, 10431, 41761, 176, 10431, 35, 83, 46909, 24, 21, 95, 10, 8018, 3795, 4, 50118, 10431, 41761, 134, 10431, 35, 370, 185, 24, 350, 444, 2128, 2150, 6, 114, 127, 2138, 56, 57, 101, 47, 77, 38, 21, 1197, 62, 6, 38, 74, 33, 1613, 5373, 4, 9427, 5, 86, 47, 4209, 69, 13495, 34156, 19, 2131, 10702, 116, 178, 77, 47, 342, 6740, 11, 69, 8492, 6, 14, 21, 95, 137, 69, 17008, 4115, 4, 50118, 10431, 41761, 176, 10431, 35, 19719, 59, 14, 3795, 6, 14, 21, 10, 410, 350, 203, 4, 125, 6918, 3829, 127, 11248, 6, 79, 460, 17216, 59, 24, 11795, 4, 50118, 10431, 41761, 134, 10431, 35, 264, 473, 33, 10, 205, 1472, 9, 12073, 4, 370, 32, 5394, 79, 6138, 47, 98, 203, 4, 1308, 2138, 8, 38, 3559, 75, 25, 593, 4, 91, 21, 195, 107, 2530, 8, 114, 37, 20711, 162, 6, 38, 460, 1299, 2581, 4, 85, 18, 205, 14, 47, 8, 6918, 32, 129, 65, 76, 4102, 11, 1046, 4, 50118, 10431, 41761, 176, 10431, 35, 264, 18, 127, 275, 1441, 6, 38, 101, 442, 69, 8265, 53, 38, 581, 393, 2581, 69, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 19860, 18, 3795, 16, 45, 10028, 19, 2150, 18, 11248, 59, 6918, 8, 3026, 2150, 45, 7, 185, 24, 350, 444, 6, 50, 24, 40, 2581, 6918, 4, 2]}.\n",
      "/home/worachotn/.local/lib/python3.10/site-packages/accelerate/accelerator.py:523: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader, processed_dataset = data_processor(logger, args, accelerator, raw_datasets, tokenizer, model)\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataloader\n",
    "train_dataset, _, _ = processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dialogue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdialogue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_dialogue: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_dialogue\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dialogue'"
     ]
    }
   ],
   "source": [
    "# for step, data in enumerate(train_dataset):\n",
    "#     # print(data)\n",
    "#     # break\n",
    "#     print('dialogue: ', len(data['dialogue']))\n",
    "#     print('negative_dialogue: ', len(data['negative_dialogue']))\n",
    "#     print('summary: ', len(data['summary']))\n",
    "#     # # print('input_ids: ', batch.input_ids[1].shape)\n",
    "#     # # print('attention_mask: ', batch.attention_mask[1].shape)\n",
    "#     # # print('negative_input_ids: ', batch.negative_input_ids[1].shape)\n",
    "#     # # print('negative_attention_mask: ', batch.negative_attention_mask[1].shape)\n",
    "#     # # print('labels: ', batch.labels[1].shape)\n",
    "#     print('='*100)\n",
    "#     break\n",
    "#     # if step == 500:\n",
    "#     #     print('dialogue: ', len(data['dialogue']))\n",
    "#     #     print('negative_dialogue: ', len(data['negative_dialogue']))\n",
    "#     #     print('summary: ', len(data['summary']))\n",
    "#     # if step == 5:\n",
    "#     #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(tokenizer.pad({\"input_ids\": train_dataset['input_ids'][0]}, padding=\"max_length\", max_length=1024, return_tensors='pt', pad_to_multiple_of=8)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  torch.Size([2, 344])\n",
      "decoder_input_ids:  torch.Size([2, 40])\n",
      "attention_mask:  torch.Size([2, 344])\n",
      "labels:  tensor([[    0, 10431, 41761,   134, 10431,    16,  1782,    10,   311,    23,\n",
      "         32474,   824,     4,   849, 41761,   134, 10431,  1519,   849, 41761,\n",
      "           176, 10431,     7,   645,    10,  9955,  1386,     9,    10,   940,\n",
      "          1155,    50, 11369,   139,     4,     2,  -100,  -100,  -100,  -100],\n",
      "        [    0, 10431, 41761,   176, 10431,  1072,     7,   907,   103,    92,\n",
      "         23353,     4,   849, 41761,   134, 10431,   924,    69,     5,  4811,\n",
      "             8,    10, 11356,  7210,  5434,     6,     8,   849, 41761,   176,\n",
      "         10431, 11703,     7,   185,   106,     4,     2,  -100,  -100,  -100]])\n",
      "input_ids:  torch.Size([2, 272])\n",
      "decoder_input_ids:  torch.Size([2, 48])\n",
      "attention_mask:  torch.Size([2, 272])\n",
      "labels:  tensor([[    0,   574,  5846,  3026,   849, 41761,   134, 10431,    59,    69,\n",
      "            92,  3269,    61,    16,   158,   728,   108,  1656,    31,    69,\n",
      "           558,     4,   849, 41761,   134, 10431,   161,   849, 41761,   134,\n",
      "         10431,    21,   628,    13,   173,     6,    53, 32593,     6,  6470,\n",
      "            21,   628,     6,   350,     4,     2,  -100,  -100],\n",
      "        [    0,   487,  9875,  3026,   849, 41761,   134, 10431,    79,   399,\n",
      "            75,   283,  2350,   142,     9, 25231,     4,   849, 41761,   134,\n",
      "         10431,  1239,  8239,   184,     4,     2,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    # print(batch)\n",
    "    print('input_ids: ', batch.input_ids.shape)\n",
    "    print('decoder_input_ids: ', batch.decoder_input_ids.shape)\n",
    "    print('attention_mask: ', batch.attention_mask.shape)\n",
    "    print('labels: ', batch.labels)\n",
    "    \n",
    "    # labels = batch.labels[0]\n",
    "    # batch_labels = batch.labels\n",
    "    # print('input_ids: ', batch.input_ids[1].shape)\n",
    "    # print('attention_mask: ', batch.attention_mask[1].shape)\n",
    "    # print('negative_input_ids: ', batch.negative_input_ids[1].shape)\n",
    "    # print('negative_attention_mask: ', batch.negative_attention_mask[1].shape)\n",
    "    # print('labels: ', batch.labels[1].shape)\n",
    "    if step == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# = = = Training Preparation = = =\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "if args.ctrlen_model: \n",
    "    no_decay_emb_matrix = [\"bias\", \"LayerNorm.weight\", \"shared\"]\n",
    "else:\n",
    "    no_decay_emb_matrix = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay_emb_matrix)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "if args.ctrlen_model:\n",
    "    if args.model_type == 'bart': \n",
    "        optimizer_grouped_parameters.extend([{\n",
    "            \"params\": model.seq2seq_model.model.shared.parameters(),\n",
    "            \"lr\": args.embedding_lr}])\n",
    "    elif args.model_type == 't5':\n",
    "        optimizer_grouped_parameters.extend([{\n",
    "            \"params\": model.seq2seq_model.shared.parameters(),\n",
    "            \"lr\": args.embedding_lr}])\n",
    "    else:\n",
    "        raise ValueError('{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "# optimizer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2023 19:14:19 - INFO - __main__ - ***** Running training *****\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Num examples = 1500\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Num Epochs = 1\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Instantaneous batch size per device = 2\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Gradient Accumulation steps = 64\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Total optimization steps = 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ef94d09c5b474d83279071c9485eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")\n",
    "\n",
    "# = = = = = = = = = = = = = = = = Train = = = = = = = = = = = = = = = = = = =\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\" Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\" Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\" Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\" Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\" Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\" Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), desc=\"Training: \", disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "\n",
    "val_results = []\n",
    "acc_losses  = []\n",
    "best_r2_f1  = None\n",
    "best_epoch  = 0\n",
    "\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError('{} model type not implemented'.format(args.model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2023 19:14:19 - INFO - __main__ - ***** Running training *****\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Num examples = 1500\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Num Epochs = 1\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Instantaneous batch size per device = 2\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Gradient Accumulation steps = 64\n",
      "10/19/2023 19:14:19 - INFO - __main__ -  Total optimization steps = 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1585fbf32de340c9bc78d17f9ed9de94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "10/19/2023 19:17:50 - INFO - __main__ - \n",
      "10/19/2023 19:17:50 - INFO - __main__ - Rouge score on val set after epoch 1\n",
      "10/19/2023 19:17:50 - INFO - root - \n",
      "10/19/2023 19:17:50 - INFO - root - \trouge-1:\tP: 51.33\tR: 42.51\tF1: 45.08\n",
      "10/19/2023 19:17:50 - INFO - root - \trouge-2:\tP: 24.21\tR: 18.96\tF1: 20.49\n",
      "10/19/2023 19:17:50 - INFO - root - \trouge-l:\tP: 52.06\tR: 44.62\tF1: 46.99\n",
      "10/19/2023 19:17:50 - INFO - root - \n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/config.json\n",
      "Configuration saved in ./output/1-bart-baseline-loss/best/generation_config.json\n",
      "Model weights saved in ./output/1-bart-baseline-loss/best/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/1-bart-baseline-loss/best/tokenizer_config.json\n",
      "Special tokens file saved in ./output/1-bart-baseline-loss/best/special_tokens_map.json\n",
      "10/19/2023 19:18:15 - INFO - __main__ - Current Best Validation Result is at epoch 1\n",
      "10/19/2023 19:18:15 - INFO - root - \n",
      "10/19/2023 19:18:15 - INFO - root - \trouge-1:\tP: 51.33\tR: 42.51\tF1: 45.08\n",
      "10/19/2023 19:18:15 - INFO - root - \trouge-2:\tP: 24.21\tR: 18.96\tF1: 20.49\n",
      "10/19/2023 19:18:15 - INFO - root - \trouge-l:\tP: 52.06\tR: 44.62\tF1: 46.99\n",
      "10/19/2023 19:18:15 - INFO - root - \n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = = = = Train = = = = = = = = = = = = = = = = = = =\n",
    "total_batch_size = args.per_device_train_batch_size * \\\n",
    "    accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\" Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\" Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(\n",
    "    f\" Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(\n",
    "    f\" Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(\n",
    "    f\" Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\" Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), desc=\"Training: \",\n",
    "                    disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "\n",
    "val_results = []\n",
    "acc_losses = []\n",
    "best_r2_f1 = None\n",
    "best_epoch = 0\n",
    "\n",
    "# edit #\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    # task_specific_params = model.module.config.task_specific_params\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    # model.module.config.update(params)\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        '{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "loss_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "last_output = None\n",
    "hidden_states = None\n",
    "\n",
    "# =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = Train =  =  =  =  =  =  =  =  =  =  =  =  =  =  =\n",
    "for epoch in range(args.num_train_epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if args.ctrlen_model:  # CTRLen model\n",
    "            outputs, loss = model(batch, tokenizer)\n",
    "        # w/ and w/o label smoothing (always better with label smoothing)\n",
    "        else:\n",
    "            if args.label_smoothing == 0:\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            else:\n",
    "                outputs = model(**batch, output_hidden_states=True)\n",
    "                last_output = outputs\n",
    "                output_logits = outputs.logits\n",
    "                hidden_states = outputs.decoder_hidden_states\n",
    "                # print(f\"logits: {output_logits.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "                # break\n",
    "                # # print(f\"hidden states: {hidden_states.shape}\")\n",
    "                # print(f\"loss: {outputs.loss}\")\n",
    "                # print(outputs.keys())\n",
    "                # print(\"=\"*100)\n",
    "                output_probs = torch.nn.functional.log_softmax(\n",
    "                    output_logits, dim=-1)\n",
    "                \n",
    "                output_probs = output_probs[:2,:,:]\n",
    "                # print(f\"output_probs: {output_probs.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "                # edit #\n",
    "                # output_probs = output_probs.view(-1,\n",
    "                #                                  model.module.config.vocab_size)\n",
    "                output_probs = output_probs.view(-1,\n",
    "                                                 model.config.vocab_size)\n",
    "                \n",
    "\n",
    "                gt_logits = batch['labels'][:2]\n",
    "                # print(f\"label: {gt_logits.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "                gt_logits = gt_logits.view(-1)\n",
    "\n",
    "                # print(f\"output_probs: {output_probs.shape}\")\n",
    "                # print(\"-\"*100)\n",
    "                # print(f\"gt_logits: {gt_logits.shape}\")\n",
    "                # print(\"=\"*100)\n",
    "                \n",
    "                loss_nll, nll = label_smoothed_nll_loss(\n",
    "                    output_probs, gt_logits, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "                \n",
    "                # cosine_loss = torch.nn.CosineEmbeddingLoss()\n",
    "                \n",
    "                # loss_cs = cosine_loss(outputs.encoder_last_hidden_state[0], outputs.encoder_last_hidden_state[1], torch.ones(outputs.encoder_last_hidden_state.size(dim=1)).to(torch.device('cuda')))\n",
    "                # positive_embeddings_1 = outputs.encoder_last_hidden_state[0]\n",
    "                # print(positive_embeddings_1.shape)\n",
    "                # positive_embeddings_2 = outputs.encoder_last_hidden_state[1]\n",
    "                # print(positive_embeddings_2.shape)\n",
    "                # negative_embeddings_1 = outputs.encoder_last_hidden_state[2]\n",
    "                # print(negative_embeddings_1.shape)\n",
    "                # negative_embeddings_2 = outputs.encoder_last_hidden_state[3]\n",
    "                # print(negative_embeddings_2.shape)\n",
    "                # print('='*100)\n",
    "                # print((-1 * torch.ones(positive_embeddings_1.size(dim=0))).shape)\n",
    "                # break\n",
    "                # Compute contrastive loss\n",
    "                # loss_1 = cosine_loss(positive_embeddings_1, negative_embeddings_1, -1 * torch.ones(positive_embeddings_1.size(dim=0)).to(device))\n",
    "                # loss_2 = cosine_loss(positive_embeddings_2, negative_embeddings_2, -1 * torch.ones(positive_embeddings_2.size(dim=0)).to(device))\n",
    "                # loss_cs = (loss_1 + loss_2) / 2\n",
    "                # print(\"loss 1: \", loss_1)\n",
    "                # print('-'*100)\n",
    "                # print(\"loss 2: \", loss_2)\n",
    "                # print('-'*100)\n",
    "                # print(\"loss: \", loss)\n",
    "                # print('='*100)\n",
    "                \n",
    "                alpha = 0.5\n",
    "                \n",
    "                # loss = loss_nll + alpha * loss_cs\n",
    "                loss = loss_nll\n",
    "                \n",
    "                # print(f\"loss_fn: {loss}\")\n",
    "                # print(\"-\"*100)\n",
    "                # print(f\"nll: {nll}\")\n",
    "                # print(\"=\"*100)\n",
    "                # break\n",
    "\n",
    "        acc_losses.append(loss.item())\n",
    "        loss_list.append(loss)\n",
    "        epoch_loss += loss.item()\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        # print(f\"loss_grad: {loss}\")\n",
    "        accelerator.backward(loss)\n",
    "        # break\n",
    "\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix(lr=lr_scheduler.get_last_lr()[\n",
    "                                     0], loss=np.mean(acc_losses[-50:]))\n",
    "            completed_steps += 1\n",
    "            train_loss_list.append(epoch_loss/len(batch))\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    # # =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = EVAL =  =  =  =  =  =  =  =  =  =  =  =  =  =  =\n",
    "    model.eval()\n",
    "    val_predict = []\n",
    "    val_groundtruth = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            # print(generated_tokens)\n",
    "            # print(\"=\"*100)\n",
    "            \n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            \n",
    "            # print(generated_tokens)\n",
    "            # print(\"=\"*100)\n",
    "            \n",
    "            labels = batch[\"labels\"]\n",
    "            if not args.pad_to_max_length:\n",
    "                # If we did not pad to max length, we need to pad the labels too\n",
    "                labels = accelerator.pad_across_processes(\n",
    "                    batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "            \n",
    "            # print(generated_tokens)\n",
    "            # print(\"=\"*100)\n",
    "            # print(labels)\n",
    "            # print(\"=\"*100)\n",
    "            # loss, _ = label_smoothed_nll_loss_gen(generated_tokens, labels, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "            # break    \n",
    "            # loss, _ = map(label_smoothed_nll_loss, generated_tokens, labels, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "            # val_loss.extend(loss)\n",
    "            \n",
    "            generated_tokens = accelerator.gather(\n",
    "                generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            if args.ignore_pad_token_for_loss:\n",
    "                # Replace -100 in the labels as we can't decode them.\n",
    "                labels = np.where(labels != -100, labels,\n",
    "                                tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "\n",
    "            # print(generated_tokens[0])\n",
    "            # print(\"=\"*100)\n",
    "            # print(labels[0])\n",
    "            # loss, _ = label_smoothed_nll_loss(generated_tokens[0], labels[0], args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "            # break \n",
    "            \n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(\n",
    "                labels, skip_special_tokens=True)\n",
    "            \n",
    "            # print(decoded_preds)\n",
    "            # print(\"=\"*100)\n",
    "            # print(decoded_labels)\n",
    "            # print(\"=\"*100)\n",
    "            \n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels)\n",
    "\n",
    "            # print(decoded_preds)\n",
    "            # print(\"=\"*100)\n",
    "            # print(decoded_labels)\n",
    "            # print(\"=\"*100)\n",
    "            # if step == 1:\n",
    "            #     break\n",
    "\n",
    "            val_predict.extend(decoded_preds)\n",
    "            val_groundtruth.extend(decoded_labels)\n",
    "\n",
    "    if args.len_output == 'real':\n",
    "        new_val_predict = []\n",
    "        for sample in val_predict:\n",
    "            try:\n",
    "                gen_sum = sample.split('Summary: ')[2]\n",
    "                new_val_predict.append(gen_sum)\n",
    "            except:\n",
    "                new_val_predict.append(sample)\n",
    "        val_predict = new_val_predict\n",
    "    else:\n",
    "        new_val_predict = val_predict\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Rouge score on val set after epoch {}\".format(epoch+1))\n",
    "    eval_results = py_rouge_scores(val_predict, val_groundtruth)\n",
    "\n",
    "    if best_r2_f1 is None:\n",
    "        best_r2_f1 = eval_results\n",
    "    if eval_results['rouge-2']['f'] >= best_r2_f1['rouge-2']['f']:\n",
    "        best_r2_f1 = eval_results\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "        os.makedirs(args.output_dir+'/best', exist_ok=True)\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            args.output_dir+'/best', save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir+'/best')\n",
    "        # save vocab\n",
    "        vocab = tokenizer.vocab.copy()\n",
    "        vocab = {k: v for k, v in sorted(\n",
    "            vocab.items(), key=lambda item: item[1])}\n",
    "        with open(args.output_dir + '/best/vocab.txt', 'w') as f:\n",
    "            for word, index in vocab.items():\n",
    "                # it lead to encoding bug on some machines, so i add this line\n",
    "                word = word.encode('ascii', 'ignore').decode('ascii')\n",
    "                f.write(str(index) + ': ' + word + '\\n')\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    logger.info(\n",
    "        \"Current Best Validation Result is at epoch {}\".format(best_epoch))\n",
    "    py_rouge_scores(None, None, best_r2_f1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/19/2023 19:18:16 - INFO - __main__ - Loading Best Result is at epoch 1 for Testing\n",
      "loading configuration file ./output/1-bart-baseline-loss/best/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 1,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 1,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading weights file ./output/1-bart-baseline-loss/best/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./output/1-bart-baseline-loss/best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ./output/1-bart-baseline-loss/best/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "10/19/2023 19:18:19 - INFO - __main__ - Collecting Testing Result...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 128,\n",
      "  \"min_length\": 1,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "10/19/2023 19:23:01 - INFO - __main__ - \n",
      "10/19/2023 19:23:01 - INFO - __main__ - ROUGE score on test set\n",
      "10/19/2023 19:23:01 - INFO - root - \n",
      "10/19/2023 19:23:01 - INFO - root - \trouge-1:\tP: 41.11\tR: 39.19\tF1: 38.58\n",
      "10/19/2023 19:23:01 - INFO - root - \trouge-2:\tP: 13.91\tR: 12.61\tF1: 12.63\n",
      "10/19/2023 19:23:01 - INFO - root - \trouge-l:\tP: 39.03\tR: 37.41\tF1: 37.15\n",
      "10/19/2023 19:23:01 - INFO - root - \n",
      "10/19/2023 19:23:01 - INFO - __main__ - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic of Summary: communication method. Length of Summary: 27. Dialogue: #Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n"
     ]
    }
   ],
   "source": [
    "# =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = Test =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = \n",
    "# load best model\n",
    "logger.info(\"Loading Best Result is at epoch {} for Testing\".format(best_epoch))\n",
    "\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "config          = config.from_pretrained(args.output_dir+'/best')\n",
    "tokenizer       = tokenizer.from_pretrained(args.output_dir+'/best', config=config)\n",
    "unwrapped_model = unwrapped_model.from_pretrained(args.output_dir+'/best', config=config)\n",
    "model           = accelerator.prepare(unwrapped_model)\n",
    "\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError('{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "# start Test \n",
    "logger.info(\"Collecting Testing Result...\")\n",
    "model.eval()\n",
    "\n",
    "test_predict     = []\n",
    "test_groundtruth = []\n",
    "for step, batch in enumerate(tqdm(test_dataloader, leave=False)):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        if not args.pad_to_max_length:\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "        generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "        labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "        if args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "\n",
    "        decoded_preds  = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        decoded_preds  = [' '.join(sent.split('\\n')) for sent in decoded_preds]\n",
    "        decoded_labels = [' '.join(sent.split('\\n')) for sent in decoded_labels]\n",
    "\n",
    "        test_predict.extend(decoded_preds)\n",
    "        test_groundtruth.extend(decoded_labels)\n",
    "\n",
    "print(raw_datasets['test']['dialogue'][0])\n",
    "\n",
    "if args.len_output == 'real':\n",
    "    new_test_predict = []\n",
    "    for sample in test_predict:\n",
    "        try:\n",
    "            gen_sum = sample.split('Summary: ')[2]\n",
    "            new_test_predict.append(gen_sum)\n",
    "        except:\n",
    "            new_test_predict.append(sample)\n",
    "    test_predict = new_test_predict\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"ROUGE score on test set\")\n",
    "test_scores = py_rouge_scores(test_predict, test_groundtruth)\n",
    "logger.info(\"\")\n",
    "\n",
    "\n",
    "# Save generated summaries\n",
    "if args.len_input == 'predict':\n",
    "    os.makedirs(args.output_dir+'/predict_gen_samples', exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(args.output_dir+'/gen_samples', exist_ok=True)\n",
    "\n",
    "for i in range(len(test_predict)):\n",
    "    test_id        = raw_datasets['test']['id'][i]\n",
    "    test_dialogue  = raw_datasets['test']['dialogue'][i]\n",
    "    test_summary   = raw_datasets['test']['summary'][i]\n",
    "    test_predict_s = test_predict[i]\n",
    "\n",
    "    if args.len_input == 'predict':\n",
    "        with open(args.output_dir+'/predict_gen_samples/'+str(test_id)+'.txt', 'w') as f:\n",
    "            test_dialogue = test_dialogue.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_dialogue)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Golden Summary:\\n')\n",
    "            test_summary = test_summary.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_summary)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Generate Summary:\\n')\n",
    "            test_predict_s = test_predict_s.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_predict_s)\n",
    "    else:\n",
    "        with open(args.output_dir+'/gen_samples/'+str(test_id)+'.txt', 'w') as f:\n",
    "            test_dialogue = test_dialogue.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_dialogue)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Golden Summary:\\n')\n",
    "            test_summary = test_summary.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_summary)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Generate Summary:\\n')\n",
    "            test_predict_s = test_predict_s.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_predict_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
