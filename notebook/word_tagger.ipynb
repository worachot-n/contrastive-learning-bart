{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8dd19a7-383f-4b8b-bf79-3a55c7a6dfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worachotn/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0)\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "from transformers.optimization import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Assume `model` is your BART model and `tokenizer` is your BART tokenizer\n",
    "input_text = \"Input text goes here.\"\n",
    "target_summary = \"Desired summary goes here.\"\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding='max_length', max_length=20, truncation=True)\n",
    "target_ids = tokenizer.encode(target_summary, return_tensors=\"pt\", padding='max_length', max_length=20, truncation=True)\n",
    "\n",
    "# Generate model predictions\n",
    "output_probs = model(input_ids).logits\n",
    "\n",
    "# Compute negative log-likelihood loss\n",
    "loss = F.nll_loss(output_probs.view(-1, output_probs.shape[-1]), target_ids.view(-1))\n",
    "\n",
    "# Backpropagate and update model parameters\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d7d87c-d1c4-4622-b962-3d1cd2daa87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.435591697692871"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e60a1402-6a2d-433d-a0ed-e017c8141924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0)\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "from transformers.optimization import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Assume `model` is your BART model and `tokenizer` is your BART tokenizer\n",
    "input_text = \"Input text goes here.\"\n",
    "target_summary = \"Desired summary goes here.\"\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding='max_length', max_length=20, truncation=True)\n",
    "target_ids = tokenizer.encode(target_summary, return_tensors=\"pt\", padding='max_length', max_length=20, truncation=True)\n",
    "\n",
    "# Generate model predictions\n",
    "output_logits = model(input_ids).logits\n",
    "output_probs = torch.nn.functional.log_softmax(output_logits, dim=-1)\n",
    "output_probs = output_probs.view(-1, model.config.vocab_size)\n",
    "\n",
    "# Compute negative log-likelihood loss\n",
    "nll_loss = F.nll_loss(output_probs.view(-1, output_probs.shape[-1]), target_ids.view(-1))\n",
    "\n",
    "# Compute MSE length loss\n",
    "generated_summary_length = len(output_probs[0])  # Length of the generated summary\n",
    "mse_length_loss = F.mse_loss(torch.tensor([generated_summary_length], dtype=torch.float), \n",
    "                             torch.tensor([target_length], dtype=torch.float))\n",
    "\n",
    "# Combine the losses (you can adjust the weighting between the two losses)\n",
    "lambda_nll = 1.0  # Weight for the NLL loss\n",
    "lambda_mse = 0.01  # Weight for the MSE length loss\n",
    "combined_loss = lambda_nll * nll_loss + lambda_mse * mse_length_loss\n",
    "\n",
    "# Backpropagate and update model parameters\n",
    "combined_loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c08a6e3-f3d1-48f7-afcb-45019771a035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa560b73-0cca-4fb1-8f18-713a16169d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 17.4297,  -1.5130,  13.2232,  ...,  -2.0266,  -2.3495,   6.0859],\n",
       "         [ 17.4297,  -1.5130,  13.2232,  ...,  -2.0266,  -2.3495,   6.0859],\n",
       "         [-11.1451,  -4.1054,  10.1617,  ...,  -4.7435,  -4.6772,  -3.2661],\n",
       "         ...,\n",
       "         [ -5.9960,  -1.9598,  13.1244,  ...,   0.0651,  -1.0115,   2.3164],\n",
       "         [ -5.7085,  -1.9905,  13.2075,  ...,   0.1223,  -0.9930,   2.3492],\n",
       "         [ -6.1833,  -2.0719,  13.1411,  ...,   0.1684,  -1.0041,   2.2347]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4db9f8d-cded-4243-a31e-8a45e2523d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.4539, -19.3966,  -4.6604,  ..., -19.9102, -20.2331, -11.7977],\n",
       "        [ -0.4539, -19.3966,  -4.6604,  ..., -19.9102, -20.2331, -11.7977],\n",
       "        [-25.5730, -18.5333,  -4.2663,  ..., -19.1714, -19.1051, -17.6941],\n",
       "        ...,\n",
       "        [-21.2853, -17.2491,  -2.1649,  ..., -15.2242, -16.3008, -12.9729],\n",
       "        [-21.0157, -17.2978,  -2.0997,  ..., -15.1849, -16.3002, -12.9580],\n",
       "        [-21.4130, -17.3016,  -2.0886,  ..., -15.0613, -16.2338, -12.9950]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "725f7267-3802-44a4-afa1-8d69fcfe1c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 28324,  7651,  4819,  1411,   259,     4,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab1da83c-52c1-442f-b252-3d573920ee04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 28324,  7651,  4819,  1411,   259,     4,     2,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_logits = target_ids[0]\n",
    "gt_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc07520c-eb9e-40be-9553-b21ef8dfd46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 28324,  7651,  4819,  1411,   259,     4,     2,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_logits = gt_logits.view(-1)\n",
    "gt_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d45385ba-495a-455a-92cf-b43248da33e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.9941, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d14b768-7ef4-483b-895f-44c0049abfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.4539, -19.3966,  -4.6604,  ..., -19.9102, -20.2331, -11.7977],\n",
       "         [ -0.4539, -19.3966,  -4.6604,  ..., -19.9102, -20.2331, -11.7977],\n",
       "         [-25.5730, -18.5333,  -4.2663,  ..., -19.1714, -19.1051, -17.6941],\n",
       "         ...,\n",
       "         [-21.2853, -17.2491,  -2.1649,  ..., -15.2242, -16.3008, -12.9729],\n",
       "         [-21.0157, -17.2978,  -2.0997,  ..., -15.1849, -16.3002, -12.9580],\n",
       "         [-21.4130, -17.3016,  -2.0886,  ..., -15.0613, -16.2338, -12.9950]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72c04b21-2f16-473f-b351-c65ad2df7be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_summary_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a90edcba-0d2c-49e7-b5b9-e4bf0f43201b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(100.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_length_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1462fdeb-f41d-4313-a4dd-582085f172b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.435591697692871"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e853ce9d-4259-4769-a091-695001de7f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.4356, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b0de942-f8c3-442d-943b-0d61474aecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/worachotn/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10b927a0-4833-40b8-8fa2-71d6b9c74323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()  # Initiate nltk lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4df43580-d7ed-4ffb-84d5-60c5f438012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_to_pos(pos):\n",
    "    \"\"\" Simple function for converting nltk pos to wordnet pos\"\"\"\n",
    "    if pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49c92261-721d-43af-a7d9-3d75d775350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(sentence):\n",
    "    \"\"\" Simple function for tokenizing text with nltk \"\"\"\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9368018-2617-4121-b7fe-e7597b736cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\" Function to lemmatize text according to the wordnet POS of each token \"\"\"\n",
    "\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    POS_assigned_text = nltk.pos_tag(tokenized_text)\n",
    "\n",
    "    available_POS = map(lambda x: (x[0], nltk_to_pos(x[1])), POS_assigned_text)\n",
    "\n",
    "    lemmatized_text = [token if pos is None\n",
    "                       else lemmatizer.lemmatize(token, pos)\n",
    "                       for token, pos in available_POS]\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97a901ba-b810-4fdf-ac2b-0274bdfeeabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'article': ['#Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir...\\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\\n',\n",
    "               '#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\\n#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\\n'],\n",
    "    'topic': ['intra-office', 'communications']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e4ce8df0-fa57-4f13-93b5-de4657091558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir...\\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\\n',\n",
       " '#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\\n#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\\n']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5e556828-bbd8-489c-8b37-b9ee4e1c5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "40e0a023-457e-4af0-ab14-bce66900113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['article'])):\n",
    "    # print(data['article'][i])\n",
    "    lemmatized_tokens.append(lemmatize_text(data['article'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a9773c75-4a6f-49fa-94c7-ad7c3f256961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['#', 'Person1', '#', ':', 'Ms.', 'Dawson', ',', 'I', 'need', 'you', 'to', 'take', 'a', 'dictation', 'for', 'me', '.', '#', 'Person2', '#', ':', 'Yes', ',', 'sir', '...', '#', 'Person1', '#', ':', 'This', 'should', 'go', 'out', 'as', 'an', 'intra-office', 'memorandum', 'to', 'all', 'employee', 'by', 'this', 'afternoon', '.', 'Are', 'you', 'ready', '?'], ['#', 'Person2', '#', ':', 'Sir', ',', 'do', 'this', 'apply', 'to', 'intra-office', 'communication', 'only', '?', 'Or', 'will', 'it', 'also', 'restrict', 'external', 'communication', '?', '#', 'Person1', '#', ':', 'It', 'should', 'apply', 'to', 'all', 'communication', ',', 'not', 'only', 'in', 'this', 'office', 'between', 'employee', ',', 'but', 'also', 'any', 'outside', 'communication', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "62a59f13-f956-4a18-8707-8eee4c5dc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bb18dbea-9060-4234-88b8-34d071c8e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['article'])):\n",
    "    # print(data['article'][i])\n",
    "    original_tokens.append(simple_tokenize(data['article'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4543b28f-b9c2-46e8-b64c-c66465d78686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['lemmatize'] = data['article'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "430918af-e3b6-4844-bff7-3ed3ea8037e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatized_tokens = lemmatize_text('#Person1#: Ms. Dawson, I need you to take a dictation for me.\\n #Person2#: Yes, sir...\\n #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "077a4840-db17-4929-9379-e2bc729c3973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lemmatized_tokens)):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e0f20183-1e06-4c54-ac4b-ab7d395be2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Person2',\n",
       " '#',\n",
       " ':',\n",
       " 'Sir',\n",
       " ',',\n",
       " 'does',\n",
       " 'this',\n",
       " 'apply',\n",
       " 'to',\n",
       " 'intra-office',\n",
       " '[TAG]communications[TAG]',\n",
       " 'only',\n",
       " '?',\n",
       " 'Or',\n",
       " 'will',\n",
       " 'it',\n",
       " 'also',\n",
       " 'restrict',\n",
       " 'external',\n",
       " '[TAG]communications[TAG]',\n",
       " '?',\n",
       " '#',\n",
       " 'Person1',\n",
       " '#',\n",
       " ':',\n",
       " 'It',\n",
       " 'should',\n",
       " 'apply',\n",
       " 'to',\n",
       " 'all',\n",
       " '[TAG]communications[TAG]',\n",
       " ',',\n",
       " 'not',\n",
       " 'only',\n",
       " 'in',\n",
       " 'this',\n",
       " 'office',\n",
       " 'between',\n",
       " 'employees',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'any',\n",
       " 'outside',\n",
       " '[TAG]communications[TAG]',\n",
       " '.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f9dbd48e-7d71-46af-bfc2-b3fc59b966d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "694aca63-2d39-4b15-8e7e-4cef6612efe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intra-office\n",
      "i\n",
      "a\n",
      "intra-office\n",
      "communications\n",
      "communication\n",
      "communication\n",
      "communication\n",
      "communication\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lemmatized_tokens)):\n",
    "    if data['topic'][i] not in data['article'][i]:\n",
    "        raise ValueError(\"Topic \" + str(assigned_topics[i]) + \"is not included in topic file.\")\n",
    "\n",
    "    # Extract all the seed words according to the corresponding topic\n",
    "    token_topics = data['topic'][i]\n",
    "    print(token_topics)\n",
    "    original_list = original_tokens[i]\n",
    "\n",
    "    for j, token in enumerate(lemmatized_tokens[i]):\n",
    "        # If the lemmatized form of the token is in topic seeds, tag the original token\n",
    "        if token.lower() in token_topics:\n",
    "            print(token.lower())\n",
    "            original_list[j] = '[TAG]' + original_list[j] + '[TAG]'\n",
    "\n",
    "    tagged_tokens.append(\" \".join(original_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e6139d77-322d-4ca4-9d75-82d73c433042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Person1 # : Ms. Dawson , [TAG]I[TAG] need you to take [TAG]a[TAG] dictation for me . # Person2 # : Yes , sir ... # Person1 # : This should go out as an [TAG]intra-office[TAG] memorandum to all employees by this afternoon . Are you ready ?',\n",
       " '# Person2 # : Sir , does this apply to intra-office [TAG]communications[TAG] only ? Or will it also restrict external [TAG]communications[TAG] ? # Person1 # : It should apply to all [TAG]communications[TAG] , not only in this office between employees , but also any outside [TAG]communications[TAG] .']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944dff0b-39a0-452d-9c00-5a4c3c8b8313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809cd9c5-88a8-4dac-9b86-417fb2496921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
